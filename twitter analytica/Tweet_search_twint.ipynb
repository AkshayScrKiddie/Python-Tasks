{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tweet_search_twint.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wszoTNyeBGyU"
      },
      "source": [
        "# Getting twitter data - Easy and without API\n",
        "\n",
        "In this short tutorial we are going to use Twint - https://pypi.org/project/twint/ to get Twitter data. This is a relatively new package that manages to get around Twitter's API. Use with care...\n",
        "\n",
        "<img src=\"https://media.giphy.com/media/SMKiEh9WDO6ze/giphy.gif\" alt=\"Girl in a jacket\" width=\"300\">\n",
        "\n",
        "We will cover 2 cases:\n",
        "\n",
        "\n",
        "\n",
        "*   Searching for tweets\n",
        "*   Cleaning and outputting as csv\n",
        "\n",
        "** Benefits** (from Twint authors)\n",
        "Some of the benefits of using Twint vs Twitter API:\n",
        "\n",
        "- Can fetch almost all Tweets (Twitter API limits to last 3200 Tweets only);\n",
        "- Fast initial setup;\n",
        "- Can be used anonymously and without Twitter sign up;\n",
        "- No rate limitations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjxttzoLKS7M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2cae7156-1c04-4231-d308-6a996ff207ee"
      },
      "source": [
        "# RUN THIS CELL ONCE AFTER REOPENING NOTEBOOK!!\n",
        "#!pip3 install -qq twint\n",
        "!pip3 install --user --upgrade git+https://github.com/twintproject/twint.git@origin/master#egg=twint\n",
        "!pip install -qq whatthelang\n",
        "\n",
        "#for scala kernel\n",
        "\n",
        "#!pip install spylon-kernel\n",
        "#!python -m spylon_kernel install\n",
        "\n",
        "\n",
        "#!ipython kernelspec list\n",
        "#!spylon -version\n",
        "\n",
        "!pip install optimus\n",
        "!pip install optimuspyspark\n",
        "\n",
        "#for Spark\n",
        "\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz\n",
        "!tar -xvf spark-3.0.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "!pip install nest_asyncio\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "#import jdc   # for %%add_to\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting twint\n",
            "  Cloning https://github.com/twintproject/twint.git (to revision origin/master) to /tmp/pip-install-5ip0yw5s/twint\n",
            "  Running command git clone -q https://github.com/twintproject/twint.git /tmp/pip-install-5ip0yw5s/twint\n",
            "\u001b[33m  WARNING: Did not find branch or tag 'origin/master', assuming revision or ref.\u001b[0m\n",
            "  Running command git checkout -q origin/master\n",
            "Collecting aiohttp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/e6/d4b6235d776c9b33f853e603efede5aac5a34f71ca9d3877adb30492eb4e/aiohttp-3.7.3-cp36-cp36m-manylinux2014_x86_64.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 13.9MB/s \n",
            "\u001b[?25hCollecting aiodns\n",
            "  Downloading https://files.pythonhosted.org/packages/da/01/8f2d49b441573fd2478833bdba91cf0b853b4c750a1fbb9e98de1b94bb22/aiodns-2.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from twint) (4.6.3)\n",
            "Collecting cchardet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/e5/a0b9edd8664ea3b0d3270c451ebbf86655ed9fc4c3e4c45b9afae9c2e382/cchardet-2.1.7-cp36-cp36m-manylinux2010_x86_64.whl (263kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 60.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: dataclasses in /usr/local/lib/python3.6/dist-packages (from twint) (0.8)\n",
            "Collecting elasticsearch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/74/054342aa07121f7c82e30ae63e3f257a793a69ddc11c5065449252dcd8af/elasticsearch-7.10.1-py2.py3-none-any.whl (322kB)\n",
            "\u001b[K     |████████████████████████████████| 327kB 60.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: pysocks in /usr/local/lib/python3.6/dist-packages (from twint) (1.7.1)\n",
            "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.6/dist-packages (from twint) (1.1.5)\n",
            "Collecting aiohttp_socks\n",
            "  Downloading https://files.pythonhosted.org/packages/14/11/67750d2d6ba48eae602392b55513441058a2a004e5271276df03d3ed6393/aiohttp_socks-0.5.5-py3-none-any.whl\n",
            "Collecting schedule\n",
            "  Downloading https://files.pythonhosted.org/packages/57/22/3a709462eb02412bd1145f6e53604f36bba191e3e4e397bea4a718fec38c/schedule-0.6.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: geopy in /usr/local/lib/python3.6/dist-packages (from twint) (1.17.0)\n",
            "Collecting fake-useragent\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/79/af647635d6968e2deb57a208d309f6069d31cb138066d7e821e575112a80/fake-useragent-0.1.11.tar.gz\n",
            "Collecting googletransx\n",
            "  Downloading https://files.pythonhosted.org/packages/27/e1/77cd530afec7944d40c5bdd260bcc111be4012b045c82d4e3ffec90b2a42/googletransx-2.4.2.tar.gz\n",
            "Collecting idna-ssl>=1.0; python_version < \"3.7\"\n",
            "  Downloading https://files.pythonhosted.org/packages/46/03/07c4894aae38b0de52b52586b24bf189bb83e4ddabfe2e2c8f2419eec6f4/idna-ssl-1.1.0.tar.gz\n",
            "Collecting yarl<2.0,>=1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/08/52b26b44bce7b818b410aee37c5e424c9ea420c557bca97dc2adac29b151/yarl-1.6.3-cp36-cp36m-manylinux2014_x86_64.whl (293kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 55.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: typing-extensions>=3.6.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp->twint) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->twint) (3.0.4)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/35/b22524d6b9cacfb4c5eff413a069bbc17c6ea628e54da5c6c989998ced5f/multidict-5.1.0-cp36-cp36m-manylinux2014_x86_64.whl (141kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 55.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->twint) (20.3.0)\n",
            "Collecting async-timeout<4.0,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
            "Collecting typing; python_version < \"3.7\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/d9/6eebe19d46bd05360c9a9aae822e67a80f9242aabbfc58b641b957546607/typing-3.7.4.3.tar.gz (78kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 12.1MB/s \n",
            "\u001b[?25hCollecting pycares>=3.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/2d/7f4984a23f6e99cf6a8b20ddc59308efb209fe81e79c97af65e9b30eefae/pycares-3.1.1-cp36-cp36m-manylinux2010_x86_64.whl (228kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 48.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: urllib3<2,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from elasticsearch->twint) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi in /usr/local/lib/python3.6/dist-packages (from elasticsearch->twint) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas->twint) (1.19.4)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->twint) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->twint) (2018.9)\n",
            "Collecting python-socks[asyncio]>=1.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b7/ce/82275ff2528b2ca3dfd397863005a79c35c832e11e0b85fce723c4ec0da7/python_socks-1.1.2-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: geographiclib<2,>=1.49 in /usr/local/lib/python3.6/dist-packages (from geopy->twint) (1.50)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from googletransx->twint) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: idna>=2.0 in /usr/local/lib/python3.6/dist-packages (from idna-ssl>=1.0; python_version < \"3.7\"->aiohttp->twint) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: cffi>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pycares>=3.0.0->aiodns->twint) (1.14.4)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->twint) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.5.0->pycares>=3.0.0->aiodns->twint) (2.20)\n",
            "Building wheels for collected packages: twint, fake-useragent, googletransx, idna-ssl, typing\n",
            "  Building wheel for twint (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for twint: filename=twint-2.1.21-cp36-none-any.whl size=38761 sha256=2fc3180b7a1cac7a11ff1b1b4bb584ad307fe7bd8f3083f8a26909023d9fd8dd\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-nqrg7zsi/wheels/4f/3b/75/62d04b3b446658ba85401e8868d3cd1d4bc22f17ad755460a6\n",
            "  Building wheel for fake-useragent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fake-useragent: filename=fake_useragent-0.1.11-cp36-none-any.whl size=13485 sha256=35b72d2cf1c23e1da11c43a676733ab40cc5e2f367dd863ba64b29a25c0bd9e4\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/63/09/d1dc15179f175357d3f5c00cbffbac37f9e8690d80545143ff\n",
            "  Building wheel for googletransx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletransx: filename=googletransx-2.4.2-cp36-none-any.whl size=15971 sha256=c4bfbfe4f148fca8f7274c2c42d437feb1c4dfb06784f49f7bca8ad0b2415aac\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/63/5f/75e7e94eb62517946116a783e4cd8970c4789c990bbc732616\n",
            "  Building wheel for idna-ssl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for idna-ssl: filename=idna_ssl-1.1.0-cp36-none-any.whl size=3163 sha256=0a61de9f41d150b4077d4fd43d9d95e2840d2c9118cf9a16fe189d3bedaf751a\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/00/b3/32d613e19e08a739751dd6bf998cfed277728f8b2127ad4eb7\n",
            "  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for typing: filename=typing-3.7.4.3-cp36-none-any.whl size=26310 sha256=3d51643871c5d9587e529ac09eb215fb83cae79840b9a59362aaa21f4c528f7e\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/04/41/8e1836e79581989c22eebac3f4e70aaac9af07b0908da173be\n",
            "Successfully built twint fake-useragent googletransx idna-ssl typing\n",
            "Installing collected packages: idna-ssl, multidict, yarl, async-timeout, aiohttp, typing, pycares, aiodns, cchardet, elasticsearch, python-socks, aiohttp-socks, schedule, fake-useragent, googletransx, twint\n",
            "\u001b[33m  WARNING: The script twint is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "Successfully installed aiodns-2.0.0 aiohttp-3.7.3 aiohttp-socks-0.5.5 async-timeout-3.0.1 cchardet-2.1.7 elasticsearch-7.10.1 fake-useragent-0.1.11 googletransx-2.4.2 idna-ssl-1.1.0 multidict-5.1.0 pycares-3.1.1 python-socks-1.1.2 schedule-0.6.0 twint-2.1.21 typing-3.7.4.3 yarl-1.6.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "typing"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 788kB 13.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 14.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 245kB 30.7MB/s \n",
            "\u001b[?25h  Building wheel for whatthelang (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for cysignals (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyfasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting optimus\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/0c/8abee22350f7a2a42933c0a8ec7c73c6741a2a4596f12ac572f4da57bb2c/Optimus-1.1.2.tar.gz (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from optimus) (1.15.0)\n",
            "Collecting click<6.0,>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/98/14966b6d772fd5fba1eb3bb34a62a7f736d609572493397cdc5715c14514/click-5.1-py2.py3-none-any.whl (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.4MB/s \n",
            "\u001b[?25hCollecting watchdog==0.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/e3/5a55d48a29300160779f0a0d2776d17c1b762a2039b36de528b093b87d5b/watchdog-0.9.0.tar.gz (85kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 9.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: Jinja2>=2.6 in /usr/local/lib/python3.6/dist-packages (from optimus) (2.11.2)\n",
            "Requirement already satisfied: Babel in /usr/local/lib/python3.6/dist-packages (from optimus) (2.9.0)\n",
            "Collecting webassets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/7c/be5d23512974c5843e94aacec163fc31539e91d1e740dd9b886f7714f9d1/webassets-2.0-py3-none-any.whl (142kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 22.2MB/s \n",
            "\u001b[?25hCollecting rcssmin\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e2/5f/852be8aa80d1c24de9b030cdb6532bc7e7a1c8461554f6edbe14335ba890/rcssmin-1.0.6.tar.gz (582kB)\n",
            "\u001b[K     |████████████████████████████████| 583kB 25.9MB/s \n",
            "\u001b[?25hCollecting jsmin\n",
            "  Downloading https://files.pythonhosted.org/packages/17/73/615d1267a82ed26cd7c124108c3c61169d8e40c36d393883eaee3a561852/jsmin-2.2.2.tar.gz\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Collecting colorlog\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/c8/c16d30bbed11a1722060014c246d124582d1f781b26f5859d8dacc3e08e1/colorlog-4.6.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from watchdog==0.9.0->optimus) (3.13)\n",
            "Collecting argh>=0.24.1\n",
            "  Downloading https://files.pythonhosted.org/packages/06/1c/e667a7126f0b84aaa1c56844337bf0ac12445d1beb9c8a6199a7314944bf/argh-0.26.2-py2.py3-none-any.whl\n",
            "Collecting pathtools>=0.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.6->optimus) (1.1.1)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.6/dist-packages (from Babel->optimus) (2018.9)\n",
            "Building wheels for collected packages: optimus, watchdog, rcssmin, jsmin, pathtools\n",
            "  Building wheel for optimus (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for optimus: filename=Optimus-1.1.2-py2.py3-none-any.whl size=74233 sha256=1636198ca1a3106e9a214af6d2ca534561d253c33bd97ee6a575d4c06cdee634\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/96/11/becee639f77468abd95f1694c03d57e9561ba9e884ae523ffe\n",
            "  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for watchdog: filename=watchdog-0.9.0-cp36-none-any.whl size=73652 sha256=f61df4ce6a4fd2bab55dc20691133b3aa110ff849111275fbe2f5e37a3a9246c\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/1d/d0/04cfe495619be2095eb8d89a31c42adb4e42b76495bc8f784c\n",
            "  Building wheel for rcssmin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rcssmin: filename=rcssmin-1.0.6-cp36-cp36m-linux_x86_64.whl size=76142 sha256=399b4fa2bfeb208a0e477e35882dba124a828c05ef06f74e7d8bdbfbf68e5bcf\n",
            "  Stored in directory: /root/.cache/pip/wheels/f5/82/85/01af385ab989b89e49bbe1d23047a8766ad43e1ebdad5084ed\n",
            "  Building wheel for jsmin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsmin: filename=jsmin-2.2.2-cp36-none-any.whl size=13921 sha256=3367b23a2687fe2856b050258d876d5931ab5f4cb4a9334b4807988d8e042aca\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/f4/de/9667d84f759289edf5442220997c6d4334637a6bb2a7b90f73\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp36-none-any.whl size=8785 sha256=48b73ba85fdcdc0c6f2e0b791568d2f64f5a43cb9c3b84c2f504f1809336d2a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "Successfully built optimus watchdog rcssmin jsmin pathtools\n",
            "\u001b[31mERROR: pip-tools 4.5.1 has requirement click>=7, but you'll have click 5.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: distributed 1.25.3 has requirement click>=6.6, but you'll have click 5.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: click, argh, pathtools, watchdog, webassets, rcssmin, jsmin, colorama, colorlog, optimus\n",
            "  Found existing installation: click 7.1.2\n",
            "    Uninstalling click-7.1.2:\n",
            "      Successfully uninstalled click-7.1.2\n",
            "Successfully installed argh-0.26.2 click-5.1 colorama-0.4.4 colorlog-4.6.2 jsmin-2.2.2 optimus-1.1.2 pathtools-0.1.2 rcssmin-1.0.6 watchdog-0.9.0 webassets-2.0\n",
            "Collecting optimuspyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/1d/24ab8c3b22ace1b812b9928ac8279b1066595777edfbd75d53787b1bc4da/optimuspyspark-2.2.32-py3-none-any.whl (65.1MB)\n",
            "\u001b[K     |████████████████████████████████| 65.1MB 43kB/s \n",
            "\u001b[?25hCollecting cryptography==2.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/18/c6557f63a6abde34707196fb2cad1c6dc0dbff25a200d5044922496668a4/cryptography-2.7-cp34-abi3-manylinux1_x86_64.whl (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 54.3MB/s \n",
            "\u001b[?25hCollecting pymongo==3.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6d/27/f30b90f40054948b32df04a8e6355946874d084ac73755986b28d3003578/pymongo-3.9.0-cp36-cp36m-manylinux1_x86_64.whl (446kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 52.0MB/s \n",
            "\u001b[?25hCollecting ipython==7.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/2e/41dce4ed129057e05a555a7f9629aa2d5f81fdcd4d16568bc24b75a1d2c9/ipython-7.5.0-py3-none-any.whl (770kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 50.9MB/s \n",
            "\u001b[?25hCollecting h2o-pysparkling-2.4==2.4.10\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/f5/400c1232720c444e56a0e997aafb8932e6e1384e228191a5bd410b657512/h2o_pysparkling_2.4-2.4.10.tar.gz (82.0MB)\n",
            "\u001b[K     |████████████████████████████████| 82.0MB 38kB/s \n",
            "\u001b[?25hCollecting psutil==5.7.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/3e/d18f2c04cf2b528e18515999b0c8e698c136db78f62df34eee89cee205f1/psutil-5.7.2.tar.gz (460kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 56.1MB/s \n",
            "\u001b[?25hCollecting findspark==1.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/b1/c8/e6e1f6a303ae5122dc28d131b5a67c5eb87cbf8f7ac5b9f87764ea1b1e1e/findspark-1.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pygments>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from optimuspyspark) (2.6.1)\n",
            "Collecting tqdm==4.28.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/55/8cb23a97301b177e9c8e3226dba45bb454411de2cbd25746763267f226c2/tqdm-4.28.1-py2.py3-none-any.whl (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.7MB/s \n",
            "\u001b[?25hCollecting seaborn==0.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/76/220ba4420459d9c4c9c9587c6ce607bf56c25b3d3d2de62056efe482dadc/seaborn-0.9.0-py3-none-any.whl (208kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 59.6MB/s \n",
            "\u001b[?25hCollecting pyarrow==0.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/25/094b122d828d24b58202712a74e661e36cd551ca62d331e388ff68bae91d/pyarrow-0.13.0-cp36-cp36m-manylinux1_x86_64.whl (48.5MB)\n",
            "\u001b[K     |████████████████████████████████| 48.5MB 62kB/s \n",
            "\u001b[?25hCollecting singleton-decorator==1.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/33/98/a8b5c919bee1152a9a1afd82014431f8db5882699754de50d1b3aba4d136/singleton-decorator-1.0.0.tar.gz\n",
            "Collecting imgkit==1.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/e5/f1/d03e16756ee4c6b89d24ae596b180e23c7a54a997a8b8ed611eb9ce3cf12/imgkit-1.0.1.tar.gz\n",
            "Collecting multipledispatch==0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/89/79/429ecef45fd5e4504f7474d4c3c3c4668c267be3370e4c2fd33e61506833/multipledispatch-0.6.0-py3-none-any.whl\n",
            "Collecting deprecated==1.2.5\n",
            "  Downloading https://files.pythonhosted.org/packages/9f/7a/003fa432f1e45625626549726c2fbb7a29baa764e9d1fdb2323a5d779f8a/Deprecated-1.2.5-py2.py3-none-any.whl\n",
            "Collecting nltk==3.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 44.0MB/s \n",
            "\u001b[?25hCollecting pyspark==2.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/64/a1df4440483df47381bbbf6a03119ef66515cf2e1a766d9369811575454b/pyspark-2.4.1.tar.gz (215.7MB)\n",
            "\u001b[K     |████████████████████████████████| 215.7MB 63kB/s \n",
            "\u001b[?25hCollecting flask==1.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7f/e7/08578774ed4536d3242b14dacb4696386634607af824ea997202cd0edb4b/Flask-1.0.2-py2.py3-none-any.whl (91kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 12.8MB/s \n",
            "\u001b[?25hCollecting matplotlib==3.0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/69/f5e05f578585ed9935247be3788b374f90701296a70c8871bcd6d21edb00/matplotlib-3.0.3-cp36-cp36m-manylinux1_x86_64.whl (13.0MB)\n",
            "\u001b[K     |████████████████████████████████| 13.0MB 254kB/s \n",
            "\u001b[?25hCollecting tensorflow==1.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/63/a9fa76de8dffe7455304c4ed635be4aa9c0bacef6e0633d87d5f54530c5c/tensorflow-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (92.5MB)\n",
            "\u001b[K     |████████████████████████████████| 92.5MB 32kB/s \n",
            "\u001b[?25hCollecting fastnumbers==2.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/85/907a9adee5b08af3d8529b845a9ab41d9a1ece94b114424a11811fc8ae08/fastnumbers-2.2.1-cp36-cp36m-manylinux1_x86_64.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.8MB/s \n",
            "\u001b[?25hCollecting pypika==0.32.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/95/4bde5976dc1fbf6d1f0cac894127f6e345e941a6f58efa5116497363e082/PyPika-0.32.0.tar.gz (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 9.0MB/s \n",
            "\u001b[?25hCollecting glom==19.10.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/e2/f41ebac95f5fb8125883fd9f4ae8116b0f5815afd98d72dbe5734f590ee4/glom-19.10.0.tar.gz (146kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 53.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: humanize==0.5.1 in /usr/local/lib/python3.6/dist-packages (from optimuspyspark) (0.5.1)\n",
            "Collecting setuptools==41.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/de/554b6310ac87c5b921bc45634b07b11394fe63bc4cb5176f5240addf18ab/setuptools-41.6.0-py2.py3-none-any.whl (582kB)\n",
            "\u001b[K     |████████████████████████████████| 583kB 55.6MB/s \n",
            "\u001b[?25hCollecting deepdiff==4.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/5c/a577c0279c36694c218988ff2e85c63b3d3b3b002034cd7f9dd361688570/deepdiff-4.0.6-py3-none-any.whl (47kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 9.4MB/s \n",
            "\u001b[?25hCollecting kombu==4.6.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/a4/1ae4a599112f5ea63a7535200a77ab5b4692b9c1e4ca1b7080a6a3cf85c1/kombu-4.6.1-py2.py3-none-any.whl (182kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 60.5MB/s \n",
            "\u001b[?25hCollecting ratelimit==2.2.1\n",
            "  Downloading https://files.pythonhosted.org/packages/ab/38/ff60c8fc9e002d50d48822cc5095deb8ebbc5f91a6b8fdd9731c87a147c9/ratelimit-2.2.1.tar.gz\n",
            "Collecting keras==2.2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 58.4MB/s \n",
            "\u001b[?25hCollecting pandas==0.24.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/74/e50234bc82c553fecdbd566d8650801e3fe2d6d8c8d940638e3d8a7c5522/pandas-0.24.2-cp36-cp36m-manylinux1_x86_64.whl (10.1MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1MB 28.0MB/s \n",
            "\u001b[?25hCollecting statsmodels==0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/d6/e9859e68e7d6c916fdff7d8e0958a7f5813485c52fc20d061273eaaddb0c/statsmodels-0.10.1-cp36-cp36m-manylinux1_x86_64.whl (8.1MB)\n",
            "\u001b[K     |████████████████████████████████| 8.1MB 27.1MB/s \n",
            "\u001b[?25hCollecting requests==2.20.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/ca/10332a30cb25b627192b4ea272c351bce3ca1091e541245cccbace6051d8/requests-2.20.0-py2.py3-none-any.whl (60kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.7MB/s \n",
            "\u001b[?25hCollecting Jinja2==2.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/e7/fd8b501e7a6dfe492a433deb7b9d833d39ca74916fa8bc63dd1a4947a671/Jinja2-2.10.1-py2.py3-none-any.whl (124kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 63.7MB/s \n",
            "\u001b[?25hCollecting backoff==1.10.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f0/32/c5dd4f4b0746e9ec05ace2a5045c1fc375ae67ee94355344ad6c7005fd87/backoff-1.10.0-py2.py3-none-any.whl\n",
            "Collecting packaging==19.0\n",
            "  Downloading https://files.pythonhosted.org/packages/91/32/58bc30e646e55eab8b21abf89e353f59c0cc02c417e42929f4a9546e1b1d/packaging-19.0-py2.py3-none-any.whl\n",
            "Collecting simplejson==3.16.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/24/c35fb1c1c315fc0fffe61ea00d3f88e85469004713dab488dee4f35b0aff/simplejson-3.16.0.tar.gz (81kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 11.8MB/s \n",
            "\u001b[?25hCollecting numpy==1.17.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/e6/c3fdc53aed9fa19d6ff3abf97dfad768ae3afce1b7431f7500000816bda5/numpy-1.17.2-cp36-cp36m-manylinux1_x86_64.whl (20.4MB)\n",
            "\u001b[K     |████████████████████████████████| 20.4MB 1.3MB/s \n",
            "\u001b[?25hCollecting ordered-set==3.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/a3/b7/d4d69641cbe707a45c23b190f2d717466ba5accc4c70b5f7a8a450387895/ordered-set-3.1.1.tar.gz\n",
            "Requirement already satisfied: six>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from cryptography==2.7->optimuspyspark) (1.15.0)\n",
            "Collecting asn1crypto>=0.21.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b5/a8/56be92dcd4a5bf1998705a9b4028249fe7c9a035b955fe93b6a3e5b829f8/asn1crypto-1.4.0-py2.py3-none-any.whl (104kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 59.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography==2.7->optimuspyspark) (1.14.4)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython==7.5.0->optimuspyspark) (4.8.0)\n",
            "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.6/dist-packages (from ipython==7.5.0->optimuspyspark) (0.17.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython==7.5.0->optimuspyspark) (0.7.5)\n",
            "Collecting prompt-toolkit<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/61/2dfea88583d5454e3a64f9308a686071d58d59a55db638268a6413e1eb6d/prompt_toolkit-2.0.10-py3-none-any.whl (340kB)\n",
            "\u001b[K     |████████████████████████████████| 348kB 60.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython==7.5.0->optimuspyspark) (4.3.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython==7.5.0->optimuspyspark) (4.4.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.6/dist-packages (from ipython==7.5.0->optimuspyspark) (0.2.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from h2o-pysparkling-2.4==2.4.10->optimuspyspark) (0.8.7)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from h2o-pysparkling-2.4==2.4.10->optimuspyspark) (0.16.0)\n",
            "Requirement already satisfied: colorama>=0.3.8 in /usr/local/lib/python3.6/dist-packages (from h2o-pysparkling-2.4==2.4.10->optimuspyspark) (0.4.4)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from seaborn==0.9.0->optimuspyspark) (1.4.1)\n",
            "Requirement already satisfied: wrapt<2,>=1 in /usr/local/lib/python3.6/dist-packages (from deprecated==1.2.5->optimuspyspark) (1.12.1)\n",
            "Collecting py4j==0.10.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 62.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask==1.0.2->optimuspyspark) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.14 in /usr/local/lib/python3.6/dist-packages (from flask==1.0.2->optimuspyspark) (1.0.1)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask==1.0.2->optimuspyspark) (5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.3->optimuspyspark) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.3->optimuspyspark) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.3->optimuspyspark) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.0.3->optimuspyspark) (0.10.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1->optimuspyspark) (0.8.1)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 9.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1->optimuspyspark) (1.32.0)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 62.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1->optimuspyspark) (0.3.3)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 53.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1->optimuspyspark) (3.12.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1->optimuspyspark) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1->optimuspyspark) (0.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1->optimuspyspark) (1.1.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1->optimuspyspark) (0.36.2)\n",
            "Collecting boltons>=19.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e1/e7979a4a6d4b296b5935e926549fff540f7670ddaf09bbf137e2b022c039/boltons-20.2.1-py2.py3-none-any.whl (170kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 62.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from glom==19.10.0->optimuspyspark) (20.3.0)\n",
            "Collecting face\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/ab/2b18c4815f3db1e04bce325271fefda55d0893738ea84e3a655218944b03/face-20.1.1.tar.gz (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.9MB/s \n",
            "\u001b[?25hCollecting jsonpickle>=1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ee/d5/1cc282dc23346a43aab461bf2e8c36593aacd34242bee1a13fa750db0cfe/jsonpickle-1.4.2-py2.py3-none-any.whl\n",
            "Collecting amqp<3.0,>=2.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/90/bb5ce93521772f083cb2d7a413bb82eda5afc62b4192adb7ea4c7b4858b9/amqp-2.6.1-py2.py3-none-any.whl (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 9.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4->optimuspyspark) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4->optimuspyspark) (2.10.0)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas==0.24.2->optimuspyspark) (2018.9)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from statsmodels==0.10.1->optimuspyspark) (0.5.1)\n",
            "Collecting idna<2.8,>=2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4b/2a/0276479a4b3caeb8a8c1af2f8e4355746a97fab05a372e4a2c6a6b876165/idna-2.7-py2.py3-none-any.whl (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 10.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.20.0->optimuspyspark) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.20.0->optimuspyspark) (2020.12.5)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.20.0->optimuspyspark) (1.24.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2==2.10.1->optimuspyspark) (1.1.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography==2.7->optimuspyspark) (2.20)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython==7.5.0->optimuspyspark) (0.6.0)\n",
            "Requirement already satisfied: parso<0.8.0,>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from jedi>=0.10->ipython==7.5.0->optimuspyspark) (0.7.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython==7.5.0->optimuspyspark) (0.2.5)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython==7.5.0->optimuspyspark) (0.2.0)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/03/b7e605db4a57c0f6fba744b11ef3ddf4ddebcada35022927a2b5fc623fdf/mock-4.0.3-py3-none-any.whl\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1->optimuspyspark) (3.3.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from jsonpickle>=1.0->deepdiff==4.0.6->optimuspyspark) (3.3.0)\n",
            "Collecting vine<5.0.0a1,>=1.1.3\n",
            "  Downloading https://files.pythonhosted.org/packages/7f/60/82c03047396126c8331ceb64da1dc52d4f1317209f32e8fe286d0c07365a/vine-1.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle>=1.0->deepdiff==4.0.6->optimuspyspark) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle>=1.0->deepdiff==4.0.6->optimuspyspark) (3.4.0)\n",
            "Building wheels for collected packages: h2o-pysparkling-2.4, psutil, singleton-decorator, imgkit, nltk, pyspark, pypika, glom, ratelimit, simplejson, ordered-set, face\n",
            "  Building wheel for h2o-pysparkling-2.4 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for h2o-pysparkling-2.4: filename=h2o_pysparkling_2.4-2.4.10-py2.py3-none-any.whl size=82017493 sha256=8e9defb2fadc9ec448a663565519b3bb31b16cf0b4825957378f96c6bc6bca9d\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/a9/11/99d4bda173f1fef47f48f3371edd90ec8b8222ec03a738f190\n",
            "  Building wheel for psutil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for psutil: filename=psutil-5.7.2-cp36-cp36m-linux_x86_64.whl size=279889 sha256=378c0046a8c0cc3b5dbc20b0eac350b7027b5e390b7f96c706745a1210af1c4f\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/a0/f5/c4fa280463e29aea07797acb5312358fefb067c1f4f98e11b1\n",
            "  Building wheel for singleton-decorator (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for singleton-decorator: filename=singleton_decorator-1.0.0-cp36-none-any.whl size=3124 sha256=f3c6063e8633a7befc56a0d168307b8383bcdfa624b9bdba8a62e3a1201cf389\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/43/87/9c4d65e727c32931aca54674ccf4f204e664306fbb507bcbd2\n",
            "  Building wheel for imgkit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imgkit: filename=imgkit-1.0.1-cp36-none-any.whl size=6587 sha256=7e5649de4c5446a3409e0cfd3e371d091d2bb0ca86d4d49e92e2b10170736f48\n",
            "  Stored in directory: /root/.cache/pip/wheels/a9/c0/97/27fd05c0ebb9fbb689a9fa2fe36cf8e9ada756f4a84a3d3454\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-cp36-none-any.whl size=1449906 sha256=c792e0a3144c89b16b87f16b08b8c5f4d6a5f7709bde3a9a9f0e9178e081888c\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-2.4.1-py2.py3-none-any.whl size=216054604 sha256=9fdf81c6b62e21f565820ef70b866f500579b65ba1e1c45754fcb465f3bd3059\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/9b/57/7984bf19763749a13eece44c3174adb6ae4bc95b920375ff50\n",
            "  Building wheel for pypika (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.32.0-py2.py3-none-any.whl size=32744 sha256=417f8fd353b33530b0d8a5c81cadbf0739d1e734b5d42ab3dac67e1d33d69a7d\n",
            "  Stored in directory: /root/.cache/pip/wheels/5d/1c/3d/377b3c7efa620b6b3d22fe61a8b59782d23137efcac0d0e268\n",
            "  Building wheel for glom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for glom: filename=glom-19.10.0-cp36-none-any.whl size=61722 sha256=c073533ae54dac2314edb87d3c8960f0b689b9f05441ac0492afd54807dbf22d\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/97/57/eb5b5a3c37e77bf5643a86db0a3bac80993ae3c5b70e1cc934\n",
            "  Building wheel for ratelimit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ratelimit: filename=ratelimit-2.2.1-cp36-none-any.whl size=5893 sha256=501619eef3691e5f5cc2b16339c0d334ec4c745c2d8f12e9915d740bfb7c90f8\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/d9/82/3c6044cf1a54aab9151612458446d9b17a38416869e1b1d9b8\n",
            "  Building wheel for simplejson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for simplejson: filename=simplejson-3.16.0-cp36-cp36m-linux_x86_64.whl size=114011 sha256=a0bc17308da270a3d136bfb853f316ff35d689e21246a93198461365a520bfe2\n",
            "  Stored in directory: /root/.cache/pip/wheels/5d/1a/1e/0350bb3df3e74215cd91325344cc86c2c691f5306eb4d22c77\n",
            "  Building wheel for ordered-set (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ordered-set: filename=ordered_set-3.1.1-py2.py3-none-any.whl size=7812 sha256=75484b7f6ddca3af31f061a2ae6fc1150f1b521f7e716b7b515688500970f521\n",
            "  Stored in directory: /root/.cache/pip/wheels/dd/70/f1/0a4953bd6d06e7dbffe8ef0197b558eaecb21fa6b043b50d73\n",
            "  Building wheel for face (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for face: filename=face-20.1.1-cp36-none-any.whl size=51079 sha256=b19817d097e2091a919717a27d3fb5d4458029ed3123218cdeb14f3e63769a17\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/3c/80/b247eb8848bb1ee6cafc32cc03670ad3e9e8825086873cc830\n",
            "Successfully built h2o-pysparkling-2.4 psutil singleton-decorator imgkit nltk pyspark pypika glom ratelimit simplejson ordered-set face\n",
            "\u001b[31mERROR: xarray 0.15.1 has requirement pandas>=0.25, but you'll have pandas 0.24.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: spacy 2.2.4 has requirement tqdm<5.0.0,>=4.38.0, but you'll have tqdm 4.28.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 3.0.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement pandas>=0.25.0, but you'll have pandas 0.24.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: mizani 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 3.0.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: mizani 0.6.0 has requirement pandas>=0.25.0, but you'll have pandas 0.24.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: jupyter-console 5.2.0 has requirement prompt-toolkit<2.0.0,>=1.0.0, but you'll have prompt-toolkit 2.0.10 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement ipython~=5.5.0, but you'll have ipython 7.5.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.1.0; python_version >= \"3.0\", but you'll have pandas 0.24.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.20.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement pandas>=1.0.4, but you'll have pandas 0.24.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement tqdm>=4.36.1, but you'll have tqdm 4.28.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: distributed 1.25.3 has requirement click>=6.6, but you'll have click 5.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: h2o-pysparkling-2-4 2.4.10 has requirement pyspark<=2.4.0,>=2.4.0, but you'll have pyspark 2.4.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: asn1crypto, cryptography, pymongo, setuptools, prompt-toolkit, ipython, idna, requests, py4j, pyspark, h2o-pysparkling-2.4, psutil, findspark, tqdm, numpy, pandas, matplotlib, seaborn, pyarrow, singleton-decorator, imgkit, multipledispatch, deprecated, nltk, Jinja2, flask, keras-applications, mock, tensorflow-estimator, tensorboard, tensorflow, fastnumbers, pypika, boltons, face, glom, ordered-set, jsonpickle, deepdiff, vine, amqp, kombu, ratelimit, keras, statsmodels, backoff, packaging, simplejson, optimuspyspark\n",
            "  Found existing installation: pymongo 3.11.2\n",
            "    Uninstalling pymongo-3.11.2:\n",
            "      Successfully uninstalled pymongo-3.11.2\n",
            "  Found existing installation: setuptools 50.3.2\n",
            "    Uninstalling setuptools-50.3.2:\n",
            "      Successfully uninstalled setuptools-50.3.2\n",
            "  Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "  Found existing installation: idna 2.10\n",
            "    Uninstalling idna-2.10:\n",
            "      Successfully uninstalled idna-2.10\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "  Found existing installation: numpy 1.19.4\n",
            "    Uninstalling numpy-1.19.4:\n",
            "      Successfully uninstalled numpy-1.19.4\n",
            "  Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "  Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "  Found existing installation: seaborn 0.11.0\n",
            "    Uninstalling seaborn-0.11.0:\n",
            "      Successfully uninstalled seaborn-0.11.0\n",
            "  Found existing installation: pyarrow 0.14.1\n",
            "    Uninstalling pyarrow-0.14.1:\n",
            "      Successfully uninstalled pyarrow-0.14.1\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "  Found existing installation: Jinja2 2.11.2\n",
            "    Uninstalling Jinja2-2.11.2:\n",
            "      Successfully uninstalled Jinja2-2.11.2\n",
            "  Found existing installation: Flask 1.1.2\n",
            "    Uninstalling Flask-1.1.2:\n",
            "      Successfully uninstalled Flask-1.1.2\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorboard 2.4.0\n",
            "    Uninstalling tensorboard-2.4.0:\n",
            "      Successfully uninstalled tensorboard-2.4.0\n",
            "  Found existing installation: tensorflow 2.4.0\n",
            "    Uninstalling tensorflow-2.4.0:\n",
            "      Successfully uninstalled tensorflow-2.4.0\n",
            "  Found existing installation: Keras 2.4.3\n",
            "    Uninstalling Keras-2.4.3:\n",
            "      Successfully uninstalled Keras-2.4.3\n",
            "  Found existing installation: statsmodels 0.10.2\n",
            "    Uninstalling statsmodels-0.10.2:\n",
            "      Successfully uninstalled statsmodels-0.10.2\n",
            "  Found existing installation: packaging 20.8\n",
            "    Uninstalling packaging-20.8:\n",
            "      Successfully uninstalled packaging-20.8\n",
            "Successfully installed Jinja2-2.10.1 amqp-2.6.1 asn1crypto-1.4.0 backoff-1.10.0 boltons-20.2.1 cryptography-2.7 deepdiff-4.0.6 deprecated-1.2.5 face-20.1.1 fastnumbers-2.2.1 findspark-1.3.0 flask-1.0.2 glom-19.10.0 h2o-pysparkling-2.4-2.4.10 idna-2.7 imgkit-1.0.1 ipython-7.5.0 jsonpickle-1.4.2 keras-2.2.4 keras-applications-1.0.8 kombu-4.6.1 matplotlib-3.0.3 mock-4.0.3 multipledispatch-0.6.0 nltk-3.4.5 numpy-1.17.2 optimuspyspark-2.2.32 ordered-set-3.1.1 packaging-19.0 pandas-0.24.2 prompt-toolkit-2.0.10 psutil-5.7.2 py4j-0.10.7 pyarrow-0.13.0 pymongo-3.9.0 pypika-0.32.0 pyspark-2.4.1 ratelimit-2.2.1 requests-2.20.0 seaborn-0.9.0 setuptools-41.6.0 simplejson-3.16.0 singleton-decorator-1.0.0 statsmodels-0.10.1 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0 tqdm-4.28.1 vine-1.3.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy",
                  "pandas",
                  "pkg_resources",
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [Waiting for headers] [Co\r                                                                               \rGet:2 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r0% [Waiting for headers] [2 InRelease 14.2 kB/88.7 kB 16%] [Connected to cloud.\r0% [Waiting for headers] [2 InRelease 43.1 kB/88.7 kB 49%] [Connected to cloud.\r                                                                               \rHit:3 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\r0% [Waiting for headers] [2 InRelease 43.1 kB/88.7 kB 49%] [Connected to cloud.\r0% [1 InRelease gpgv 15.9 kB] [Waiting for headers] [2 InRelease 43.1 kB/88.7 k\r                                                                               \rGet:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "\r0% [1 InRelease gpgv 15.9 kB] [4 InRelease 12.3 kB/88.7 kB 14%] [2 InRelease 51\r                                                                               \rGet:5 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "\r0% [1 InRelease gpgv 15.9 kB] [4 InRelease 14.2 kB/88.7 kB 16%] [2 InRelease 75\r                                                                               \rGet:6 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Ign:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:9 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,700 kB]\n",
            "Ign:10 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Get:12 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release [564 B]\n",
            "Get:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:14 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release.gpg [833 B]\n",
            "Get:15 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [870 kB]\n",
            "Get:16 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [40.7 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [237 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,816 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [15.3 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,372 kB]\n",
            "Get:21 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [46.5 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [53.8 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [266 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,244 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,136 kB]\n",
            "Ign:26 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:26 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [506 kB]\n",
            "Get:27 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Packages [66.1 kB]\n",
            "Fetched 11.7 MB in 2s (5,927 kB/s)\n",
            "Reading package lists... Done\n",
            "spark-3.0.1-bin-hadoop3.2/\n",
            "spark-3.0.1-bin-hadoop3.2/RELEASE\n",
            "spark-3.0.1-bin-hadoop3.2/examples/\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/FPGrowthExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/GBTExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/ALSExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/KMeansExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionSummaryExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/TokenizerExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/UnaryTransformerExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/Word2VecExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/LinearSVCExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/CorrelationExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/BucketizerExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/ImputerExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/StringIndexerExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/NGramExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/IsotonicRegressionExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeRegressionExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/OneHotEncoderExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/ChiSquareTestExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/PCAExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/StopWordsRemoverExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/DeveloperApiExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/LDAExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/StandardScalerExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeRegressorExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/RFormulaExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/MultilayerPerceptronClassifierExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestRegressorExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/OneVsRestExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/PolynomialExpansionExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/ChiSqSelectorExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/MinMaxScalerExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/RobustScalerExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/FeatureHasherExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionWithElasticNetExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/FMClassifierExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/IndexToStringExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionWithElasticNetExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/GeneralizedLinearRegressionExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/BucketedRandomProjectionLSHExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/BisectingKMeansExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/MinHashLSHExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestClassifierExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeClassifierExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/PowerIterationClusteringExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/CountVectorizerExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/TfIdfExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/VectorSlicerExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/MulticlassLogisticRegressionWithElasticNetExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/MaxAbsScalerExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/BinarizerExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/DataFrameExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/InteractionExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/VectorIndexerExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaCrossValidationExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/EstimatorTransformerParamExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/PipelineExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/NaiveBayesExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/VectorSizeHintExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/PrefixSpanExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/SQLTransformerExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/SummarizerExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/DCTExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/AFTSurvivalRegressionExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaTrainValidationSplitExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeClassificationExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/QuantileDiscretizerExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/NormalizerExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/FMRegressorExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/VectorAssemblerExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/ElementwiseProductExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/GaussianMixtureExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SkewedGroupByTest.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkALS.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/KernelDensityEstimationExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/FPGrowthExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/LBFGSExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/SVDExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/KMeansExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/Word2VecExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/StratifiedSamplingExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/LogisticRegressionWithLBFGSExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/IsotonicRegressionExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/RandomRDDGeneration.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRegressionExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/SummaryStatisticsExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingTestExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/PMMLModelExportExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassificationMetricsExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/MulticlassMetricsExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/LDAExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingKMeansExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/StandardScalerExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/SimpleFPGrowth.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnRowMatrixExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/Correlations.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/ChiSqSelectorExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/SparseNaiveBayes.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLinearRegressionExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/SVMWithSGDExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/MultivariateSummarizer.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnySVD.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/TFIDFExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostedTreesRunner.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/MovieLensALS.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnSourceVectorExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/MultiLabelMetricsExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/RecommendationExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/BisectingKMeansExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingClassificationExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/PowerIterationClusteringExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/LatentDirichletAllocationExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/RankingMetricsExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/DenseKMeans.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestClassificationExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnyPCA.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/AbstractParams.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/NaiveBayesExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/PrefixSpanExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingRegressionExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRunner.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLogisticRegression.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingKolmogorovSmirnovTestExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassification.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeClassificationExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/SampledRDDs.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/AssociationRulesExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/CosineSimilarity.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestRegressionExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/CorrelationsExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/NormalizerExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/ElementwiseProductExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/GaussianMixtureExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ExceptionHandlingTest.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/BroadcastTest.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/HdfsTest.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkHdfsLR.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/SqlNetworkWordCount.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/QueueStream.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/RawNetworkGrep.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKafkaWordCount.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/StatefulNetworkWordCount.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKerberizedKafkaWordCount.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/HdfsWordCount.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/CustomReceiver.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/StreamingExamples.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewStream.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewGenerator.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/RecoverableNetworkWordCount.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/NetworkWordCount.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/LocalLR.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkKMeans.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/MultiBroadcastTest.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkLR.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/LocalPi.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/Analytics.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/SynthBenchmark.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/ComprehensiveExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/AggregateMessagesExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/SSSPExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/PageRankExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/ConnectedComponentsExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/LiveJournalPageRank.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/TriangleCountingExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/LocalFileLR.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkPageRank.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/RDDRelation.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/SimpleTypedAggregator.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/streaming/\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCount.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCountWindowed.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredSessionization.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKafkaWordCount.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKerberizedKafkaWordCount.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/hive/\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/hive/SparkHiveExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedScalar.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedTypedAggregation.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedUntypedAggregation.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SimpleSkewedGroupByTest.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/LocalKMeans.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/AccumulatorMetricsTest.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/pythonconverters/\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/pythonconverters/AvroConverters.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/LocalALS.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/GroupByTest.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/LogQuery.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/DriverSubmissionTest.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/DFSReadWriteTest.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkRemoteFileTest.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkTC.scala\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/resources/\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/resources/users.avro\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/resources/full_user.avsc\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/resources/dir1/\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/resources/dir1/file1.parquet\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/resources/dir1/file3.json\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/resources/dir1/dir2/\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/resources/dir1/dir2/file2.parquet\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/resources/users.orc\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/resources/kv1.txt\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/resources/users.parquet\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/resources/people.txt\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/resources/user.avsc\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/resources/people.csv\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/resources/employees.json\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/resources/people.json\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scripts/\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/scripts/getGpusResources.sh\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaInteractionExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaRFormulaExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaNaiveBayesExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaImputerExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaMultilayerPerceptronClassifierExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestRegressorExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestClassifierExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaTfIdfExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeClassifierExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaCountVectorizerExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaRobustScalerExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionWithElasticNetExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSizeHintExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaMulticlassLogisticRegressionWithElasticNetExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaKMeansExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorIndexerExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaBinarizerExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaPolynomialExpansionExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaStringIndexerExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaPCAExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaOneVsRestExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaLabeledDocument.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaFeatureHasherExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaFMRegressorExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorAssemblerExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaSummarizerExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaBisectingKMeansExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaPrefixSpanExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketedRandomProjectionLSHExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaWord2VecExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSlicerExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearRegressionWithElasticNetExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaIndexToStringExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeClassificationExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaDCTExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaTokenizerExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaALSExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaFMClassifierExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaEstimatorTransformerParamExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaSQLTransformerExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaTrainValidationSplitExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaQuantileDiscretizerExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaCorrelationExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaNGramExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeRegressorExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaCrossValidationExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaIsotonicRegressionExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaMinMaxScalerExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSquareTestExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaElementwiseProductExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaAFTSurvivalRegressionExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaStandardScalerExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeRegressionExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaOneHotEncoderExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaDocument.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaFPGrowthExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaLDAExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaMinHashLSHExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSqSelectorExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaNormalizerExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaGaussianMixtureExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaStopWordsRemoverExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaPipelineExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaGeneralizedLinearRegressionExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearSVCExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionSummaryExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaPowerIterationClusteringExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaMaxAbsScalerExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketizerExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/JavaHdfsLR.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaNaiveBayesExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVDExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaKernelDensityEstimationExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingKolmogorovSmirnovTestExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaKMeansExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaPCAExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestClassificationExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaCorrelationsExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaLatentDirichletAllocationExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaBisectingKMeansExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaPrefixSpanExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaSummaryStatisticsExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVMWithSGDExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingRegressionExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeClassificationExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaBinaryClassificationMetricsExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaSimpleFPGrowth.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaALS.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaIsotonicRegressionExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaElementwiseProductExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingClassificationExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaMulticlassClassificationMetricsExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaLogisticRegressionWithLBFGSExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaRecommendationExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaAssociationRulesExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeRegressionExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaLBFGSExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaRankingMetricsExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaMultiLabelClassificationMetricsExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaStratifiedSamplingExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaChiSqSelectorExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaGaussianMixtureExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestRegressionExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaStreamingTestExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaPowerIterationClusteringExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/JavaLogQuery.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaDirectKafkaWordCount.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecoverableNetworkWordCount.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaQueueStream.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecord.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaCustomReceiver.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaNetworkWordCount.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaSqlNetworkWordCount.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaDirectKerberizedKafkaWordCount.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaStatefulNetworkWordCount.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/JavaStatusTrackerDemo.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/streaming/\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCount.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredSessionization.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredKerberizedKafkaWordCount.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCountWindowed.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredKafkaWordCount.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/hive/\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/hive/JavaSparkHiveExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedScalar.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedUntypedAggregation.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedTypedAggregation.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/JavaTC.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/JavaWordCount.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/JavaPageRank.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/JavaSparkPi.java\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/r/\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/r/ml/\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/r/ml/lda.R\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/r/ml/logit.R\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/r/ml/gaussianMixture.R\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/r/ml/survreg.R\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/r/ml/glm.R\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/r/ml/powerIterationClustering.R\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/r/ml/als.R\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/r/ml/ml.R\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/r/ml/fpm.R\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/r/ml/bisectingKmeans.R\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/r/ml/isoreg.R\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/r/ml/mlp.R\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/r/ml/kmeans.R\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/r/ml/randomForest.R\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/r/ml/gbt.R\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/r/ml/kstest.R\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/r/ml/svmLinear.R\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/r/ml/decisionTree.R\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/r/ml/naiveBayes.R\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/r/ml/prefixSpan.R\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/r/data-manipulation.R\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/r/streaming/\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/r/streaming/structured_network_wordcount.R\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/r/RSparkSQLExample.R\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/r/dataframe.R\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/bisecting_k_means_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/stopwords_remover_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/chisq_selector_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/cross_validator.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/vector_indexer_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/binarizer_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/tokenizer_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/quantile_discretizer_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/vector_size_hint_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/elementwise_product_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/tf_idf_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/pca_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/chi_square_test_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/correlation_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/rformula_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/fpgrowth_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/dataframe_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/train_validation_split.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/fm_classifier_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/index_to_string_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/gradient_boosted_tree_regressor_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/logistic_regression_summary_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/feature_hasher_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/vector_slicer_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/kmeans_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/robust_scaler_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/als_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/decision_tree_classification_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/generalized_linear_regression_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/fm_regressor_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/multiclass_logistic_regression_with_elastic_net.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/imputer_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/interaction_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/naive_bayes_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/string_indexer_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/gradient_boosted_tree_classifier_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/sql_transformer.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/logistic_regression_with_elastic_net.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/decision_tree_regression_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/lda_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/min_hash_lsh_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/estimator_transformer_param_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/gaussian_mixture_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/prefixspan_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/summarizer_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/max_abs_scaler_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/aft_survival_regression.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/onehot_encoder_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/normalizer_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/random_forest_regressor_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/random_forest_classifier_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/vector_assembler_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/power_iteration_clustering_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/linear_regression_with_elastic_net.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/count_vectorizer_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/standard_scaler_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/bucketizer_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/multilayer_perceptron_classification.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/dct_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/linearsvc.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/polynomial_expansion_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/isotonic_regression_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/n_gram_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/word2vec_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/min_max_scaler_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/bucketed_random_projection_lsh_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/one_vs_rest_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/ml/pipeline_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/mllib/\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/mllib/bisecting_k_means_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/mllib/multi_class_metrics_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/mllib/regression_metrics_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/mllib/correlations.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/mllib/elementwise_product_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/mllib/svd_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/mllib/tf_idf_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/mllib/kernel_density_estimation_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/mllib/pca_rowmatrix_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/mllib/ranking_metrics_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/mllib/gradient_boosting_regression_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/mllib/fpgrowth_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/mllib/random_rdd_generation.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/mllib/word2vec.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/mllib/linear_regression_with_sgd_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/mllib/logistic_regression_with_lbfgs_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/mllib/kmeans.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/mllib/streaming_k_means_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/mllib/decision_tree_classification_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/mllib/correlations_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/mllib/naive_bayes_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/mllib/decision_tree_regression_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/mllib/sampled_rdds.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/mllib/gaussian_mixture_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/mllib/hypothesis_testing_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/mllib/streaming_linear_regression_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/mllib/stratified_sampling_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/mllib/gaussian_mixture_model.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/mllib/normalizer_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/mllib/summary_statistics_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/mllib/svm_with_sgd_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/mllib/logistic_regression.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/mllib/power_iteration_clustering_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/mllib/standard_scaler_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/mllib/random_forest_classification_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/mllib/gradient_boosting_classification_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/mllib/random_forest_regression_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/mllib/isotonic_regression_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/mllib/k_means_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/mllib/word2vec_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/mllib/binary_classification_metrics_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/mllib/multi_label_metrics_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/mllib/hypothesis_testing_kolmogorov_smirnov_test_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/mllib/latent_dirichlet_allocation_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/mllib/recommendation_example.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/transitive_closure.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/parquet_inputformat.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/streaming/\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/streaming/sql_network_wordcount.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/streaming/queue_stream.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/streaming/recoverable_network_wordcount.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/streaming/network_wordjoinsentiments.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/streaming/network_wordcount.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/streaming/hdfs_wordcount.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/streaming/stateful_network_wordcount.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/kmeans.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/als.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/sql/\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/sql/streaming/\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/sql/streaming/structured_kafka_wordcount.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/sql/streaming/structured_network_wordcount_windowed.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/sql/streaming/structured_network_wordcount.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/sql/basic.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/sql/datasource.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/sql/hive.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/sql/arrow.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/pagerank.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/logistic_regression.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/wordcount.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/status_api_demo.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/avro_inputformat.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/pi.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/src/main/python/sort.py\n",
            "spark-3.0.1-bin-hadoop3.2/examples/jars/\n",
            "spark-3.0.1-bin-hadoop3.2/examples/jars/spark-examples_2.12-3.0.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/examples/jars/scopt_2.12-3.7.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/data/\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/ridge-data/\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/ridge-data/lpsa.data\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/streaming_kmeans_data_test.txt\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/sample_multiclass_classification_data.txt\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/sample_isotonic_regression_libsvm_data.txt\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/sample_svm_data.txt\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/iris_libsvm.txt\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/sample_lda_data.txt\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/sample_lda_libsvm_data.txt\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/sample_movielens_data.txt\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/sample_libsvm_data.txt\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/sample_binary_classification_data.txt\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/sample_fpgrowth.txt\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/images/\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/images/partitioned/\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/images/partitioned/cls=multichannel/\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/images/partitioned/cls=multichannel/date=2018-01/\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/images/partitioned/cls=multichannel/date=2018-01/BGRA_alpha_60.png\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/images/partitioned/cls=multichannel/date=2018-01/BGRA.png\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/images/partitioned/cls=multichannel/date=2018-02/\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/images/partitioned/cls=multichannel/date=2018-02/chr30.4.184.jpg\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/images/partitioned/cls=multichannel/date=2018-02/grayscale.jpg\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/date=2018-01/\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/date=2018-01/not-image.txt\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/date=2018-01/29.5.a_b_EGDP022204.jpg\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/date=2018-02/\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/date=2018-02/DP153539.jpg\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/date=2018-02/54893.jpg\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/date=2018-02/DP802813.jpg\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/images/origin/\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/images/origin/multi-channel/\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/images/origin/multi-channel/BGRA_alpha_60.png\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/images/origin/multi-channel/chr30.4.184.jpg\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/images/origin/multi-channel/BGRA.png\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/images/origin/multi-channel/grayscale.jpg\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/images/origin/license.txt\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/images/origin/kittens/\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/images/origin/kittens/not-image.txt\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/images/origin/kittens/DP153539.jpg\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/images/origin/kittens/54893.jpg\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/images/origin/kittens/29.5.a_b_EGDP022204.jpg\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/images/origin/kittens/DP802813.jpg\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/images/license.txt\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/gmm_data.txt\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/sample_linear_regression_data.txt\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/kmeans_data.txt\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/pagerank_data.txt\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/pic_data.txt\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/sample_kmeans_data.txt\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/als/\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/als/sample_movielens_ratings.txt\n",
            "spark-3.0.1-bin-hadoop3.2/data/mllib/als/test.data\n",
            "spark-3.0.1-bin-hadoop3.2/data/streaming/\n",
            "spark-3.0.1-bin-hadoop3.2/data/streaming/AFINN-111.txt\n",
            "spark-3.0.1-bin-hadoop3.2/data/graphx/\n",
            "spark-3.0.1-bin-hadoop3.2/data/graphx/followers.txt\n",
            "spark-3.0.1-bin-hadoop3.2/data/graphx/users.txt\n",
            "spark-3.0.1-bin-hadoop3.2/yarn/\n",
            "spark-3.0.1-bin-hadoop3.2/yarn/spark-3.0.1-yarn-shuffle.jar\n",
            "spark-3.0.1-bin-hadoop3.2/bin/\n",
            "spark-3.0.1-bin-hadoop3.2/bin/spark-class.cmd\n",
            "spark-3.0.1-bin-hadoop3.2/bin/run-example\n",
            "spark-3.0.1-bin-hadoop3.2/bin/run-example.cmd\n",
            "spark-3.0.1-bin-hadoop3.2/bin/spark-class\n",
            "spark-3.0.1-bin-hadoop3.2/bin/spark-class2.cmd\n",
            "spark-3.0.1-bin-hadoop3.2/bin/find-spark-home\n",
            "spark-3.0.1-bin-hadoop3.2/bin/spark-submit\n",
            "spark-3.0.1-bin-hadoop3.2/bin/pyspark\n",
            "spark-3.0.1-bin-hadoop3.2/bin/find-spark-home.cmd\n",
            "spark-3.0.1-bin-hadoop3.2/bin/docker-image-tool.sh\n",
            "spark-3.0.1-bin-hadoop3.2/bin/spark-shell.cmd\n",
            "spark-3.0.1-bin-hadoop3.2/bin/spark-sql.cmd\n",
            "spark-3.0.1-bin-hadoop3.2/bin/load-spark-env.cmd\n",
            "spark-3.0.1-bin-hadoop3.2/bin/sparkR2.cmd\n",
            "spark-3.0.1-bin-hadoop3.2/bin/beeline.cmd\n",
            "spark-3.0.1-bin-hadoop3.2/bin/pyspark.cmd\n",
            "spark-3.0.1-bin-hadoop3.2/bin/spark-shell2.cmd\n",
            "spark-3.0.1-bin-hadoop3.2/bin/spark-sql2.cmd\n",
            "spark-3.0.1-bin-hadoop3.2/bin/sparkR\n",
            "spark-3.0.1-bin-hadoop3.2/bin/spark-sql\n",
            "spark-3.0.1-bin-hadoop3.2/bin/spark-shell\n",
            "spark-3.0.1-bin-hadoop3.2/bin/spark-submit.cmd\n",
            "spark-3.0.1-bin-hadoop3.2/bin/load-spark-env.sh\n",
            "spark-3.0.1-bin-hadoop3.2/bin/beeline\n",
            "spark-3.0.1-bin-hadoop3.2/bin/sparkR.cmd\n",
            "spark-3.0.1-bin-hadoop3.2/bin/pyspark2.cmd\n",
            "spark-3.0.1-bin-hadoop3.2/bin/spark-submit2.cmd\n",
            "spark-3.0.1-bin-hadoop3.2/sbin/\n",
            "spark-3.0.1-bin-hadoop3.2/sbin/stop-thriftserver.sh\n",
            "spark-3.0.1-bin-hadoop3.2/sbin/stop-slaves.sh\n",
            "spark-3.0.1-bin-hadoop3.2/sbin/spark-config.sh\n",
            "spark-3.0.1-bin-hadoop3.2/sbin/start-mesos-dispatcher.sh\n",
            "spark-3.0.1-bin-hadoop3.2/sbin/stop-mesos-dispatcher.sh\n",
            "spark-3.0.1-bin-hadoop3.2/sbin/stop-master.sh\n",
            "spark-3.0.1-bin-hadoop3.2/sbin/start-thriftserver.sh\n",
            "spark-3.0.1-bin-hadoop3.2/sbin/spark-daemons.sh\n",
            "spark-3.0.1-bin-hadoop3.2/sbin/stop-all.sh\n",
            "spark-3.0.1-bin-hadoop3.2/sbin/start-master.sh\n",
            "spark-3.0.1-bin-hadoop3.2/sbin/start-slave.sh\n",
            "spark-3.0.1-bin-hadoop3.2/sbin/stop-slave.sh\n",
            "spark-3.0.1-bin-hadoop3.2/sbin/start-slaves.sh\n",
            "spark-3.0.1-bin-hadoop3.2/sbin/start-history-server.sh\n",
            "spark-3.0.1-bin-hadoop3.2/sbin/stop-mesos-shuffle-service.sh\n",
            "spark-3.0.1-bin-hadoop3.2/sbin/stop-history-server.sh\n",
            "spark-3.0.1-bin-hadoop3.2/sbin/start-all.sh\n",
            "spark-3.0.1-bin-hadoop3.2/sbin/start-mesos-shuffle-service.sh\n",
            "spark-3.0.1-bin-hadoop3.2/sbin/slaves.sh\n",
            "spark-3.0.1-bin-hadoop3.2/sbin/spark-daemon.sh\n",
            "spark-3.0.1-bin-hadoop3.2/README.md\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-zstd.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-scala.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-JTransforms.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-pyrolite.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-mustache.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-scopt.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-paranamer.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-istack-commons-runtime.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-vis-timeline.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-bootstrap.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-javax-transaction-transaction-api.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-arpack.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-xmlenc.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-zstd-jni.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-spire.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-cloudpickle.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-automaton.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-sbt-launch-lib.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-slf4j.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-jquery.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-jakarta.activation-api.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-matchMedia-polyfill.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-kryo.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-JLargeArrays.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-netlib.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-jakarta.xml.bind-api.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-graphlib-dot.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-jakarta-ws-rs-api\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-f2j.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-d3.min.js.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-javolution.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-modernizr.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-join.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-py4j.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-javassist.html\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-respond.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-jakarta-annotation-api\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-datatables.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-re2j.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-janino.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-CC0.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-json-formatter.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-heapq.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-jodd.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-jsp-api.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-reflectasm.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-jline.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-pmml-model.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-machinist.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-sorttable.js.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-leveldbjni.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-protobuf.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-dagre-d3.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-jaxb-runtime.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-antlr.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-minlog.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-AnchorJS.txt\n",
            "spark-3.0.1-bin-hadoop3.2/licenses/LICENSE-dnsjava.txt\n",
            "spark-3.0.1-bin-hadoop3.2/LICENSE\n",
            "spark-3.0.1-bin-hadoop3.2/kubernetes/\n",
            "spark-3.0.1-bin-hadoop3.2/kubernetes/dockerfiles/\n",
            "spark-3.0.1-bin-hadoop3.2/kubernetes/dockerfiles/spark/\n",
            "spark-3.0.1-bin-hadoop3.2/kubernetes/dockerfiles/spark/entrypoint.sh\n",
            "spark-3.0.1-bin-hadoop3.2/kubernetes/dockerfiles/spark/bindings/\n",
            "spark-3.0.1-bin-hadoop3.2/kubernetes/dockerfiles/spark/bindings/R/\n",
            "spark-3.0.1-bin-hadoop3.2/kubernetes/dockerfiles/spark/bindings/R/Dockerfile\n",
            "spark-3.0.1-bin-hadoop3.2/kubernetes/dockerfiles/spark/bindings/python/\n",
            "spark-3.0.1-bin-hadoop3.2/kubernetes/dockerfiles/spark/bindings/python/Dockerfile\n",
            "spark-3.0.1-bin-hadoop3.2/kubernetes/dockerfiles/spark/Dockerfile\n",
            "spark-3.0.1-bin-hadoop3.2/kubernetes/tests/\n",
            "spark-3.0.1-bin-hadoop3.2/kubernetes/tests/pyfiles.py\n",
            "spark-3.0.1-bin-hadoop3.2/kubernetes/tests/worker_memory_check.py\n",
            "spark-3.0.1-bin-hadoop3.2/kubernetes/tests/py_container_checks.py\n",
            "spark-3.0.1-bin-hadoop3.2/R/\n",
            "spark-3.0.1-bin-hadoop3.2/R/lib/\n",
            "spark-3.0.1-bin-hadoop3.2/R/lib/sparkr.zip\n",
            "spark-3.0.1-bin-hadoop3.2/R/lib/SparkR/\n",
            "spark-3.0.1-bin-hadoop3.2/R/lib/SparkR/html/\n",
            "spark-3.0.1-bin-hadoop3.2/R/lib/SparkR/html/00Index.html\n",
            "spark-3.0.1-bin-hadoop3.2/R/lib/SparkR/html/R.css\n",
            "spark-3.0.1-bin-hadoop3.2/R/lib/SparkR/help/\n",
            "spark-3.0.1-bin-hadoop3.2/R/lib/SparkR/help/aliases.rds\n",
            "spark-3.0.1-bin-hadoop3.2/R/lib/SparkR/help/AnIndex\n",
            "spark-3.0.1-bin-hadoop3.2/R/lib/SparkR/help/paths.rds\n",
            "spark-3.0.1-bin-hadoop3.2/R/lib/SparkR/help/SparkR.rdx\n",
            "spark-3.0.1-bin-hadoop3.2/R/lib/SparkR/help/SparkR.rdb\n",
            "spark-3.0.1-bin-hadoop3.2/R/lib/SparkR/DESCRIPTION\n",
            "spark-3.0.1-bin-hadoop3.2/R/lib/SparkR/Meta/\n",
            "spark-3.0.1-bin-hadoop3.2/R/lib/SparkR/Meta/features.rds\n",
            "spark-3.0.1-bin-hadoop3.2/R/lib/SparkR/Meta/nsInfo.rds\n",
            "spark-3.0.1-bin-hadoop3.2/R/lib/SparkR/Meta/links.rds\n",
            "spark-3.0.1-bin-hadoop3.2/R/lib/SparkR/Meta/Rd.rds\n",
            "spark-3.0.1-bin-hadoop3.2/R/lib/SparkR/Meta/package.rds\n",
            "spark-3.0.1-bin-hadoop3.2/R/lib/SparkR/Meta/hsearch.rds\n",
            "spark-3.0.1-bin-hadoop3.2/R/lib/SparkR/INDEX\n",
            "spark-3.0.1-bin-hadoop3.2/R/lib/SparkR/profile/\n",
            "spark-3.0.1-bin-hadoop3.2/R/lib/SparkR/profile/general.R\n",
            "spark-3.0.1-bin-hadoop3.2/R/lib/SparkR/profile/shell.R\n",
            "spark-3.0.1-bin-hadoop3.2/R/lib/SparkR/R/\n",
            "spark-3.0.1-bin-hadoop3.2/R/lib/SparkR/R/SparkR\n",
            "spark-3.0.1-bin-hadoop3.2/R/lib/SparkR/R/SparkR.rdx\n",
            "spark-3.0.1-bin-hadoop3.2/R/lib/SparkR/R/SparkR.rdb\n",
            "spark-3.0.1-bin-hadoop3.2/R/lib/SparkR/worker/\n",
            "spark-3.0.1-bin-hadoop3.2/R/lib/SparkR/worker/worker.R\n",
            "spark-3.0.1-bin-hadoop3.2/R/lib/SparkR/worker/daemon.R\n",
            "spark-3.0.1-bin-hadoop3.2/R/lib/SparkR/NAMESPACE\n",
            "spark-3.0.1-bin-hadoop3.2/R/lib/SparkR/tests/\n",
            "spark-3.0.1-bin-hadoop3.2/R/lib/SparkR/tests/testthat/\n",
            "spark-3.0.1-bin-hadoop3.2/R/lib/SparkR/tests/testthat/test_basic.R\n",
            "spark-3.0.1-bin-hadoop3.2/NOTICE\n",
            "spark-3.0.1-bin-hadoop3.2/jars/\n",
            "spark-3.0.1-bin-hadoop3.2/jars/pyrolite-4.30.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/hadoop-yarn-common-3.2.0.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/parquet-hadoop-1.10.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/hive-shims-2.3.7.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/jackson-databind-2.10.0.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/spire_2.12-0.17.0-M1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/jul-to-slf4j-1.7.30.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/nimbus-jose-jwt-4.41.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/stax2-api-3.1.4.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/json4s-ast_2.12-3.6.6.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/jersey-media-jaxb-2.30.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/commons-configuration2-2.1.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/xbean-asm7-shaded-4.15.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/spark-catalyst_2.12-3.0.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/commons-logging-1.1.3.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/curator-recipes-2.13.0.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/RoaringBitmap-0.7.45.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/commons-daemon-1.0.13.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/jakarta.inject-2.6.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/flatbuffers-java-1.9.0.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/guice-servlet-4.0.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/kubernetes-model-common-4.9.2.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/json-1.8.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/jaxb-runtime-2.3.2.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/mesos-1.4.0-shaded-protobuf.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/orc-core-1.5.10.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/chill_2.12-0.9.5.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/leveldbjni-all-1.8.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/kryo-shaded-4.0.2.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/curator-framework-2.13.0.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/jackson-annotations-2.10.0.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/hk2-api-2.6.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/hk2-locator-2.6.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/commons-beanutils-1.9.4.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/osgi-resource-locator-1.0.3.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/antlr4-runtime-4.7.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/jersey-client-2.30.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/json4s-scalap_2.12-3.6.6.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/accessors-smart-1.2.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/joda-time-2.10.5.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/okio-1.15.0.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/libthrift-0.12.0.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/gson-2.2.4.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/hadoop-hdfs-client-3.2.0.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/hadoop-mapreduce-client-core-3.2.0.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/guice-4.0.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/hadoop-yarn-client-3.2.0.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/jta-1.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/parquet-jackson-1.10.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/super-csv-2.2.0.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/commons-httpclient-3.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/woodstox-core-5.0.3.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/javax.jdo-3.2.0-m3.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/kerb-admin-1.0.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/hadoop-client-3.2.0.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/httpclient-4.5.6.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/datanucleus-api-jdo-4.2.4.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/aopalliance-1.0.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/slf4j-api-1.7.30.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/univocity-parsers-2.9.0.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/scala-compiler-2.12.10.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/transaction-api-1.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/hive-serde-2.3.7.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/JLargeArrays-1.5.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/libfb303-0.9.3.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/hadoop-auth-3.2.0.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/jakarta.ws.rs-api-2.1.6.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/hive-shims-0.23-2.3.7.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/dnsjava-2.1.7.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/snappy-java-1.1.7.5.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/compress-lzf-1.0.3.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/commons-io-2.4.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/snakeyaml-1.24.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/spark-hive-thriftserver_2.12-3.0.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/breeze-macros_2.12-1.0.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/metrics-json-4.1.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/hadoop-mapreduce-client-common-3.2.0.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/okhttp-2.7.5.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/hk2-utils-2.6.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/bonecp-0.8.0.RELEASE.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/automaton-1.11-8.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/avro-ipc-1.8.2.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/re2j-1.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/spark-core_2.12-3.0.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/okhttp-3.12.6.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/kerb-client-1.0.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/jackson-datatype-jsr310-2.10.3.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/metrics-jvm-4.1.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/avro-mapred-1.8.2-hadoop2.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/guava-14.0.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/algebra_2.12-2.0.0-M2.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/protobuf-java-2.5.0.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/stream-2.9.6.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/spark-tags_2.12-3.0.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/metrics-core-4.1.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/kerb-server-1.0.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/hadoop-annotations-3.2.0.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/stax-api-1.0.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/jackson-module-scala_2.12-2.10.0.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/commons-cli-1.2.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/spark-kvstore_2.12-3.0.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/aopalliance-repackaged-2.6.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/arrow-vector-0.15.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/datanucleus-core-4.1.17.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/spark-network-shuffle_2.12-3.0.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/lz4-java-1.7.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/hadoop-yarn-api-3.2.0.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/hive-jdbc-2.3.7.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/janino-3.0.16.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/ivy-2.4.0.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/commons-lang3-3.9.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/arpack_combined_all-0.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/spark-hive_2.12-3.0.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/kerby-asn1-1.0.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/derby-10.12.1.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/commons-text-1.6.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/macro-compat_2.12-1.1.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/hive-beeline-2.3.7.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/slf4j-log4j12-1.7.30.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/scala-collection-compat_2.12-2.1.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/hive-llap-common-2.3.7.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/jakarta.xml.bind-api-2.3.2.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/hive-shims-common-2.3.7.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/hadoop-yarn-server-common-3.2.0.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/generex-1.0.2.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/objenesis-2.5.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/commons-math3-3.4.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/metrics-graphite-4.1.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/logging-interceptor-3.12.6.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/commons-codec-1.10.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/HikariCP-2.5.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/jackson-core-asl-1.9.13.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/kerb-core-1.0.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/core-1.1.2.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/spark-tags_2.12-3.0.1-tests.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/arrow-format-0.15.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/orc-shims-1.5.10.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/jdo-api-3.0.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/log4j-1.2.17.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/paranamer-2.8.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/jodd-core-3.5.2.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/kubernetes-model-4.9.2.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/kerb-crypto-1.0.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/spire-util_2.12-0.17.0-M1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/htrace-core4-4.1.0-incubating.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/jcl-over-slf4j-1.7.30.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/shapeless_2.12-2.3.3.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/hive-metastore-2.3.7.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/parquet-common-1.10.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/zstd-jni-1.4.4-3.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/hive-vector-code-gen-2.3.7.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/hadoop-mapreduce-client-jobclient-3.2.0.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/jakarta.validation-api-2.0.2.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/jersey-container-servlet-core-2.30.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/scala-parser-combinators_2.12-1.1.2.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/javassist-3.25.0-GA.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/hadoop-yarn-registry-3.2.0.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/commons-lang-2.6.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/hive-shims-scheduler-2.3.7.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/spire-macros_2.12-0.17.0-M1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/parquet-format-2.4.0.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/hadoop-common-3.2.0.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/commons-compiler-3.0.16.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/opencsv-2.3.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/jakarta.annotation-api-1.3.5.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/jpam-1.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/kerby-pkix-1.0.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/ehcache-3.3.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/py4j-0.10.9.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/istack-commons-runtime-3.0.8.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/spire-platform_2.12-0.17.0-M1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/datanucleus-rdbms-4.1.19.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/spark-repl_2.12-3.0.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/spark-mesos_2.12-3.0.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/json4s-jackson_2.12-3.6.6.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/jackson-module-paranamer-2.10.0.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/hive-cli-2.3.7.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/jackson-jaxrs-base-2.9.5.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/javax.inject-1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/jersey-container-servlet-2.30.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/jersey-common-2.30.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/commons-pool-1.5.4.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/shims-0.7.45.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/jackson-core-2.10.0.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/jline-2.14.6.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/kerb-identity-1.0.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/jsp-api-2.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/jakarta.activation-api-1.2.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/httpcore-4.4.12.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/ST4-4.0.4.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/metrics-jmx-4.1.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/kerby-xdr-1.0.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/jackson-module-jaxb-annotations-2.10.0.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/jersey-hk2-2.30.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/orc-mapreduce-1.5.10.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/jersey-server-2.30.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/JTransforms-3.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/javax.servlet-api-3.1.0.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/spark-graphx_2.12-3.0.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.0.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/arrow-memory-0.15.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/spark-sketch_2.12-3.0.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/hive-exec-2.3.7-core.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/machinist_2.12-0.6.8.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/antlr-runtime-3.5.2.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/spark-streaming_2.12-3.0.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/commons-net-3.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/audience-annotations-0.5.0.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/velocity-1.5.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/curator-client-2.13.0.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/kerby-config-1.0.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/hive-storage-api-2.7.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/kerb-common-1.0.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/commons-compress-1.8.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/breeze_2.12-1.0.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/netty-all-4.1.47.Final.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/zjsonpatch-0.3.0.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/chill-java-0.9.5.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/activation-1.1.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/spark-mllib_2.12-3.0.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/spark-launcher_2.12-3.0.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/spark-network-common_2.12-3.0.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/kerby-util-1.0.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/hadoop-yarn-server-web-proxy-3.2.0.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/jcip-annotations-1.0-1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/jackson-mapper-asl-1.9.13.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/geronimo-jcache_1.0_spec-1.0-alpha-1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/token-provider-1.0.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/oro-2.0.8.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/jackson-jaxrs-json-provider-2.9.5.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/kubernetes-client-4.9.2.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/kerb-simplekdc-1.0.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/jsr305-3.0.0.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/jaxb-api-2.2.11.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/spark-yarn_2.12-3.0.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/spark-mllib-local_2.12-3.0.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/json4s-core_2.12-3.6.6.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/xz-1.5.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/zookeeper-3.4.14.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/threeten-extra-1.5.0.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/parquet-column-1.10.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/kerb-util-1.0.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/spark-sql_2.12-3.0.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/commons-crypto-1.0.0.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/javolution-5.5.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/aircompressor-0.10.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/scala-library-2.12.10.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/hive-common-2.3.7.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/json-smart-2.3.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/commons-dbcp-1.4.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/parquet-encoding-1.10.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/commons-collections-3.2.2.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/scala-xml_2.12-1.2.0.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/jackson-dataformat-yaml-2.10.0.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/cats-kernel_2.12-2.0.0-M4.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/scala-reflect-2.12.10.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/avro-1.8.2.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/minlog-1.3.0.jar\n",
            "spark-3.0.1-bin-hadoop3.2/jars/spark-kubernetes_2.12-3.0.1.jar\n",
            "spark-3.0.1-bin-hadoop3.2/python/\n",
            "spark-3.0.1-bin-hadoop3.2/python/setup.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/ml/\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/ml/functions.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/ml/util.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/ml/regression.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/ml/evaluation.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/ml/feature.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/ml/base.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/ml/tuning.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/ml/clustering.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/ml/stat.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/ml/image.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/ml/fpm.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/ml/recommendation.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/ml/pipeline.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/ml/param/\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/ml/param/_shared_params_code_gen.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/ml/param/__init__.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/ml/param/shared.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/ml/common.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/ml/linalg/\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/ml/linalg/__init__.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/ml/classification.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/ml/tree.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/ml/wrapper.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/ml/__init__.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/ml/tests/\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/ml/tests/test_wrapper.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/ml/tests/test_tuning.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/ml/tests/test_base.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/ml/tests/test_evaluation.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/ml/tests/test_linalg.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/ml/tests/test_image.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/ml/tests/test_persistence.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/ml/tests/test_feature.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/ml/tests/test_param.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/ml/tests/test_algorithms.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/ml/tests/test_pipeline.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/ml/tests/__init__.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/ml/tests/test_training_summary.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/ml/tests/test_stat.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/statcounter.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/status.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/java_gateway.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/mllib/\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/mllib/util.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/mllib/regression.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/mllib/evaluation.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/mllib/feature.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/mllib/clustering.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/mllib/stat/\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/mllib/stat/test.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/mllib/stat/_statistics.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/mllib/stat/distribution.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/mllib/stat/KernelDensity.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/mllib/stat/__init__.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/mllib/fpm.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/mllib/recommendation.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/mllib/common.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/mllib/linalg/\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/mllib/linalg/distributed.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/mllib/linalg/__init__.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/mllib/classification.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/mllib/tree.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/mllib/__init__.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/mllib/random.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/mllib/tests/\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/mllib/tests/test_util.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/mllib/tests/test_linalg.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/mllib/tests/test_feature.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/mllib/tests/test_algorithms.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/mllib/tests/__init__.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/mllib/tests/test_streaming_algorithms.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/mllib/tests/test_stat.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/resource.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/util.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/daemon.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/context.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/broadcast.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/streaming/\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/streaming/util.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/streaming/listener.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/streaming/kinesis.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/streaming/context.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/streaming/__init__.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/streaming/tests/\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/streaming/tests/test_kinesis.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/streaming/tests/test_context.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/streaming/tests/test_dstream.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/streaming/tests/test_listener.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/streaming/tests/__init__.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/testing/\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/testing/mllibutils.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/testing/utils.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/testing/mlutils.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/testing/sqlutils.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/testing/streamingutils.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/testing/__init__.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/shell.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/heapq3.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/version.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/rddsampler.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/worker.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/cloudpickle.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/functions.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/column.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/readwriter.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/utils.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/context.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/avro/\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/avro/functions.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/avro/__init__.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/window.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/session.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/types.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/group.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/catalog.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/udf.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/pandas/\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/pandas/group_ops.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/pandas/serializers.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/pandas/functions.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/pandas/utils.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/pandas/typehints.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/pandas/types.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/pandas/__init__.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/pandas/map_ops.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/__init__.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/conf.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/streaming.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/tests/\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/tests/test_column.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/tests/test_arrow.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/tests/test_conf.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/tests/test_pandas_grouped_map.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/tests/test_catalog.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/tests/test_pandas_cogrouped_map.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/tests/test_pandas_udf_typehints.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/tests/test_serde.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/tests/test_types.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/tests/test_pandas_udf_scalar.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/tests/test_utils.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/tests/test_pandas_map.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/tests/test_streaming.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/tests/test_functions.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/tests/test_context.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/tests/test_group.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/tests/test_datasources.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/tests/test_readwriter.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/tests/test_pandas_udf_window.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/tests/test_udf.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/tests/__init__.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/tests/test_session.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/tests/test_pandas_udf.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/tests/test_pandas_udf_grouped_agg.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/tests/test_dataframe.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/join.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/accumulators.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/resultiterable.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/profiler.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/taskcontext.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/storagelevel.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/traceback_utils.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/files.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/_globals.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/shuffle.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/__init__.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/rdd.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/find_spark_home.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/conf.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/tests/\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/tests/test_taskcontext.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/tests/test_appsubmit.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/tests/test_util.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/tests/test_conf.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/tests/test_daemon.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/tests/test_pin_thread.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/tests/test_profiler.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/tests/test_shuffle.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/tests/test_readwrite.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/tests/test_serializers.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/tests/test_broadcast.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/tests/test_rddbarrier.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/tests/test_context.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/tests/test_join.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/tests/__init__.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/tests/test_worker.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pyspark/tests/test_rdd.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_coverage/\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_coverage/sitecustomize.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_coverage/coverage_daemon.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_coverage/conf/\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_coverage/conf/spark-defaults.conf\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/SimpleHTTPServer.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/userlibrary.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/userlib-0.1.zip\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/sql/\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/sql/streaming/\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/sql/streaming/text-test.txt\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/sql/people_array_utf16le.json\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/sql/ages.csv\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/sql/ages_newlines.csv\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/sql/text-test.txt\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/sql/people1.json\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/sql/orc_partitioned/\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=0/\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=0/c=0/\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=0/c=0/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=0/c=0/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/sql/orc_partitioned/_SUCCESS\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=1/\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=1/c=1/\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=1/c=1/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=1/c=1/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/_metadata\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/_SUCCESS\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/.part-r-00005.gz.parquet.crc\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/part-r-00005.gz.parquet\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00004.gz.parquet.crc\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00002.gz.parquet\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00002.gz.parquet.crc\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00004.gz.parquet\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=9/\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/.part-r-00007.gz.parquet.crc\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/part-r-00007.gz.parquet\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/_common_metadata\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2014/\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2014/month=9/\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/.part-r-00008.gz.parquet.crc\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/part-r-00008.gz.parquet\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/sql/people_array.json\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/sql/people.json\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/hello/\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/hello/hello.txt\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/hello/sub_hello/\n",
            "spark-3.0.1-bin-hadoop3.2/python/test_support/hello/sub_hello/sub_hello.txt\n",
            "spark-3.0.1-bin-hadoop3.2/python/README.md\n",
            "spark-3.0.1-bin-hadoop3.2/python/run-tests-with-coverage\n",
            "spark-3.0.1-bin-hadoop3.2/python/MANIFEST.in\n",
            "spark-3.0.1-bin-hadoop3.2/python/.coveragerc\n",
            "spark-3.0.1-bin-hadoop3.2/python/docs/\n",
            "spark-3.0.1-bin-hadoop3.2/python/docs/pyspark.mllib.rst\n",
            "spark-3.0.1-bin-hadoop3.2/python/docs/index.rst\n",
            "spark-3.0.1-bin-hadoop3.2/python/docs/pyspark.sql.rst\n",
            "spark-3.0.1-bin-hadoop3.2/python/docs/_static/\n",
            "spark-3.0.1-bin-hadoop3.2/python/docs/_static/pyspark.css\n",
            "spark-3.0.1-bin-hadoop3.2/python/docs/_static/copybutton.js\n",
            "spark-3.0.1-bin-hadoop3.2/python/docs/_static/pyspark.js\n",
            "spark-3.0.1-bin-hadoop3.2/python/docs/make.bat\n",
            "spark-3.0.1-bin-hadoop3.2/python/docs/pyspark.resource.rst\n",
            "spark-3.0.1-bin-hadoop3.2/python/docs/pyspark.rst\n",
            "spark-3.0.1-bin-hadoop3.2/python/docs/make2.bat\n",
            "spark-3.0.1-bin-hadoop3.2/python/docs/_templates/\n",
            "spark-3.0.1-bin-hadoop3.2/python/docs/_templates/layout.html\n",
            "spark-3.0.1-bin-hadoop3.2/python/docs/pyspark.streaming.rst\n",
            "spark-3.0.1-bin-hadoop3.2/python/docs/Makefile\n",
            "spark-3.0.1-bin-hadoop3.2/python/docs/conf.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/docs/pyspark.ml.rst\n",
            "spark-3.0.1-bin-hadoop3.2/python/lib/\n",
            "spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip\n",
            "spark-3.0.1-bin-hadoop3.2/python/lib/pyspark.zip\n",
            "spark-3.0.1-bin-hadoop3.2/python/lib/PY4J_LICENSE.txt\n",
            "spark-3.0.1-bin-hadoop3.2/python/run-tests.py\n",
            "spark-3.0.1-bin-hadoop3.2/python/pylintrc\n",
            "spark-3.0.1-bin-hadoop3.2/python/run-tests\n",
            "spark-3.0.1-bin-hadoop3.2/python/setup.cfg\n",
            "spark-3.0.1-bin-hadoop3.2/python/.gitignore\n",
            "spark-3.0.1-bin-hadoop3.2/conf/\n",
            "spark-3.0.1-bin-hadoop3.2/conf/fairscheduler.xml.template\n",
            "spark-3.0.1-bin-hadoop3.2/conf/log4j.properties.template\n",
            "spark-3.0.1-bin-hadoop3.2/conf/spark-defaults.conf.template\n",
            "spark-3.0.1-bin-hadoop3.2/conf/metrics.properties.template\n",
            "spark-3.0.1-bin-hadoop3.2/conf/spark-env.sh.template\n",
            "spark-3.0.1-bin-hadoop3.2/conf/slaves.template\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.6/dist-packages (1.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWox3Oc9LYD0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c882f15-0333-4433-85e2-95254c7f41ad"
      },
      "source": [
        "!pip uninstall pyarrow -y\n",
        "!pip3 install pyarrow\n",
        "!pip3 install --uninstall pandas\n",
        "!pip3 install --install pandas\n",
        "!pip3 install --upgrade pandas"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling pyarrow-2.0.0:\n",
            "  Successfully uninstalled pyarrow-2.0.0\n",
            "Collecting pyarrow\n",
            "  Using cached https://files.pythonhosted.org/packages/d7/e1/27958a70848f8f7089bff8d6ebe42519daf01f976d28b481e1bfd52c8097/pyarrow-2.0.0-cp36-cp36m-manylinux2014_x86_64.whl\n",
            "Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.6/dist-packages (from pyarrow) (1.17.2)\n",
            "\u001b[31mERROR: optimuspyspark 2.2.32 has requirement pandas==0.24.2, but you'll have pandas 1.1.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: optimuspyspark 2.2.32 has requirement pyarrow==0.13.0, but you'll have pyarrow 2.0.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pyarrow\n",
            "Successfully installed pyarrow-2.0.0\n",
            "\n",
            "Usage:   \n",
            "  pip3 install [options] <requirement specifier> [package-index-options] ...\n",
            "  pip3 install [options] -r <requirements file> [package-index-options] ...\n",
            "  pip3 install [options] [-e] <vcs project url> ...\n",
            "  pip3 install [options] [-e] <local project path> ...\n",
            "  pip3 install [options] <archive url/path> ...\n",
            "\n",
            "no such option: --uninstall\n",
            "/usr/local/lib/python3.6/dist-packages/pip/_internal/commands/install.py:283: UserWarning: Disabling all use of wheels due to the use of --build-options / --global-options / --install-options.\n",
            "  cmdoptions.check_install_build_global(options)\n",
            "\u001b[31mERROR: You must give at least one requirement to install (see \"pip help install\")\u001b[0m\n",
            "Requirement already up-to-date: pandas in /usr/local/lib/python3.6/dist-packages (1.1.5)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.17.2)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5G3trcmmnamP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5830e0fd-5276-4d4e-a974-020434dd6a5d"
      },
      "source": [
        "#!pip install --target=$nb_path jdc --upgrade\n",
        "!pip install nest_asyncio\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.6/dist-packages (1.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvKySTqNfhS5"
      },
      "source": [
        "#import jdc   # for %%add_to\n",
        "import nest_asyncio\n",
        "import pandas as pd\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5AEUhz321J1"
      },
      "source": [
        "import findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop3.2\"\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cOZctsVmALk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d9434a4-94e2-4687-aac5-46f4ff48b8cf"
      },
      "source": [
        "import twint\n",
        "c = twint.Config()\n",
        "a=str(input(\"Enter:\"))\n",
        "\n",
        "c.Store_object = True\n",
        "c.Pandas =True\n",
        "\n",
        "c.Search = a\n",
        "c.Limit = 10\n",
        "c.Lang = 'en'\n",
        "\n",
        "Tweets_df = twint.storage.panda.Tweets_df\n",
        "twint.run.Search(c)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter:uber\n",
            "1340920678481063937 2020-12-21 07:22:57 +0000 <UberINSupport> @SGoyal3189 Authorization holds are voided by Uber within hours. However, they are refunded to your payment method within 7 business days, depending on your payment methods policy. Appreciate your understanding.  https://t.co/1WqzzOmdKe\n",
            "1340920675930759168 2020-12-21 07:22:56 +0000 <EuskalValley> Uber vende su unidad de vehículos autónomos por 3.300 millones de euros a la startup Aurora  https://t.co/snjALteMFQ\n",
            "1340920625515307010 2020-12-21 07:22:44 +0000 <meghnabali> A mate sent me this screenshot from a Sydney Uber and rideshare drivers Facebook page. It’s the racism for me 🙃 @UberEats  https://t.co/FVuH6RmWhT\n",
            "1340920604363350016 2020-12-21 07:22:39 +0000 <SoftcoreV> hey all my followrrs!! got free uber eats codes!! dm for pm!\n",
            "1340920584247664641 2020-12-21 07:22:35 +0000 <_mmiaaax> HAHAHA JUAL KASUT BAYAR UBER LEH TAK\n",
            "1340920521190383617 2020-12-21 07:22:20 +0000 <UE_moppy> @Uber_osaka2613 かくれんぼ('_'?)\n",
            "1340920495777017863 2020-12-21 07:22:13 +0000 <kgmtjd1> @meganen11771 Uber eatsだ\n",
            "1340920461002231808 2020-12-21 07:22:05 +0000 <j3zc4> E eu que conheci meu futuro professor de inglês no Uber kkkk, aí aí eu o próprio meme de \"não vou falar nada quando entrar no Uber\"🤡\n",
            "1340920432770334720 2020-12-21 07:21:58 +0000 <LeSwankJuice> When I hop in a Uber today and the driver not black #December21  https://t.co/USgYIDfvS8\n",
            "1340920429981130752 2020-12-21 07:21:58 +0000 <ChemchemLinda> @LCP @six_valerie vive uber est et les streaming, franchement c quoi ce chantage\n",
            "1340920401698893825 2020-12-21 07:21:51 +0000 <cupomrappi010> @edbergjoan @BK_NMateus 🍟🍕🍔🍟🍕🍔🍟🍕🍔🍟🍕🍔🍟🍕🍔  Cupom Rappi !    💸45 R$ NO PEDIDO 📛  + 150 R$ DE FRETE GRÁTIS 💸  APROVEITEM A PROMOÇÃO.🛑 LINK NO PERFIL🛑  💸 Cupom: hrp32015999 💸  ✅+45 R$ OFF✅  🍟🍕🍔🍟🍕🍔🍟🍕🍔🍟🍕🍔🍟🍕🍔 cupom ifood  e uber eats e 99\n",
            "1340920400180432897 2020-12-21 07:21:51 +0000 <22222_212121> @uber_moe なぬっ！！夜の部も楽しんできてください☺️✨\n",
            "1340920389837463557 2020-12-21 07:21:48 +0000 <hayratomaz> Pra fechar o domingo pegamos o mesmo uber que levou a gente no upa no domingo passado 😂\n",
            "1340920361794351105 2020-12-21 07:21:42 +0000 <Stumaiza_TuMee> @Miss_Oliveee Lwena you leave far you'd Uber\n",
            "1340920342785765376 2020-12-21 07:21:37 +0000 <cagalhovei> indo pra praia e fumei um com o Uber\n",
            "1340920341170941952 2020-12-21 07:21:37 +0000 <BP_Skhosana> @OluwaseunMakin4 @Protee_sa Fix your spelling, cause now I need ancestors to explain what you need exactly cause I have corolla I wanna throw into the Uber industry.  https://t.co/Bp28Y5AaI2\n",
            "1340920328881537024 2020-12-21 07:21:34 +0000 <Eien_Noachanosi> 今日はUberするかぁ！！\n",
            "1340920326016827392 2020-12-21 07:21:33 +0000 <EXILEPARKER> @riptidesb @Uber_Support i hope y'all are having fun !! :D\n",
            "1340920292525273088 2020-12-21 07:21:25 +0000 <ryancrawcour> @jimkat2 The same (mostly terrible) drivers often drive (the same mostly terrible  cars) for multiple services. I miss US Uber.  Probably only miss that and Amazon though.\n",
            "1340920270937210881 2020-12-21 07:21:20 +0000 <YoungestBode> When you’re faded and trying to see if it’s your Uber\n",
            "1340920253392564224 2020-12-21 07:21:16 +0000 <Uber_Support> @BloodyPeachh We definitely want to get this sorted out. Please send us a DM with the phone number that is associated with your Uber account, so we can further assist.\n",
            "1340920243372367872 2020-12-21 07:21:13 +0000 <barelyhermes> I really don't understand why my laid back ass decided to go out yesterday and almost curfew hour  I ended up paying 1190 for an Uber. I'm still stressed\n",
            "1340920229996556289 2020-12-21 07:21:10 +0000 <saesaenosae> 今日Uberのサイト見てて、カートに商品入れて会計の画面で評価（チップ）の画面が出てきました！！クレジットを登録したからかな？\n",
            "1340920219024449536 2020-12-21 07:21:07 +0000 <itsangiehehe> @UberEats yo Uber wtf is going on me and my bestie are hungry and your app is not working😭\n",
            "1340920208102330368 2020-12-21 07:21:05 +0000 <rei_roku> @AIhXeeSgaNHxqM6 騙された〜🤣時間2なら全然良いじゃないですかw  DiDiのインセは20件ごとに1000円とか金額が増えていくのは良いですね。これならイケる！って気になりそう。 Uberと比べて50円くらい高い？🤔\n",
            "1340920194676482048 2020-12-21 07:21:02 +0000 <karinaamoreno> @Renefabian6 Es que te mamas también jajajajajaja. Yo quiero 🥺 mándalos en un Uber express JAJAJA\n",
            "1340920175642734592 2020-12-21 07:20:57 +0000 <SirAlexas> Uber Eats 😍😍😍😍\n",
            "1340920164179689473 2020-12-21 07:20:54 +0000 <BietjieG> I’m going to order Uber Eats for Christmas 💔\n",
            "1340920159775715329 2020-12-21 07:20:53 +0000 <thaaliamartin> Fui enganada pela @Uber que me deu 2 meses de @telecine gratis, mas fui cadastrar e me cobrou 🤡🤡🤡🤡🤡\n",
            "1340920113952940037 2020-12-21 07:20:42 +0000 <oliveirxglau> eu não paguei 21,00 de uber do iguaçu ate o caravelas não né\n",
            "1340920065961562112 2020-12-21 07:20:31 +0000 <eat33930535> @Uber_SAGAMI 他人のは被りたくないですよね🤮 売れるんですかね･･･\n",
            "1340920033233555456 2020-12-21 07:20:23 +0000 <aluizax_> eu sou completamente derretida pelo bjorn, uber e ivar\n",
            "1340920031232704514 2020-12-21 07:20:23 +0000 <orangina_san> @ryo1umidanji @takeshi_uber  https://t.co/249mKBI148 下の方に24.25は店内飲食禁止にするとの表記が出てました😳💦\n",
            "1340920026090459136 2020-12-21 07:20:21 +0000 <christhekeele> @coryodaniel @MattOswaltVA Man goes to doctor, says \"I'm so depressed about the $600 stimulus. I may starve to death!\"  Doctor says, \"Use it to call Uber Eats!\"  \"But doctor, I AM the Uber Eats!\"  ~scene~\n",
            "1340920004288507904 2020-12-21 07:20:16 +0000 <3O5HOLLAND> @riptidesb @Uber_Support BIE AKSMDLD\n",
            "1340919975746236416 2020-12-21 07:20:09 +0000 <sharamamas> miss telling my bitches the uber here😩\n",
            "1340919952170123265 2020-12-21 07:20:04 +0000 <oyu_oyu_sayu> @UBER_eats_ramen @yettmh0526 普通に考えたら、開始と完了しまくって１日100件いけますもんね笑\n",
            "1340919872616722432 2020-12-21 07:19:45 +0000 <kshkr> あなたも #UberEats ( #ウーバーイーツ )で働いてみませんか❓Uber Eatsは2016年にスタートしたフードデリバリーサービスです！  https://t.co/qv0G7ZvIqO\n",
            "1340919821341356038 2020-12-21 07:19:33 +0000 <tibi_ponta> 2020年6月更新！【Uber Eats】ボーナス貰える招待コード  eats-fakebv  これの入力してみてください！ ##ウーバーイーツクーポン\n",
            "1340919821177778176 2020-12-21 07:19:33 +0000 <UberEatsJPNTYO> @Uber_leeeeee 最近、新橋から、豊洲も可能性ありますよ😭  https://t.co/1OHU7tfPET\n",
            "1340919741725204482 2020-12-21 07:19:14 +0000 <AppFraud> @DavidBrianJoll1 EU can’t do deals with private companies. Here in UK Uber has vanished everywhere - only left in London now. And even there there drivers are quitting by 600 - 800 a week. Be none left in 12 months.\n",
            "1340919684678508545 2020-12-21 07:19:00 +0000 <costinhaz1k4> 4 da manhã na rua e o Uber há 20 min kkkkkkkk\n",
            "1340919664759758848 2020-12-21 07:18:55 +0000 <Uber_Support> @theobserver83 Here to help. Please send us a DM with the mobile number associated with your Uber account as well as any information regarding this promotion like the promo code or a screenshot of the promo offer you received via email or in-app, so that we can assist you.\n",
            "1340919648808660992 2020-12-21 07:18:52 +0000 <xx10xx01xx> 一回ぐらいUber頼みたいからアプリ落としたけど頼めるお店少なくない？ほぼチェーン店だけどww 田舎ww\n",
            "1340919572384366592 2020-12-21 07:18:33 +0000 <krlkimvtc> a única coisa boa é as vezes o uber deixar mais barato a corrida 😔😔😔\n",
            "1340919548388761600 2020-12-21 07:18:28 +0000 <cupomrappi010> @BUeOBk0nWjQrVJe @gilmaraemmanu 🍟🍕🍔🍟🍕🍔🍟🍕🍔🍟🍕🍔🍟🍕🍔  Cupom Rappi !    💸45 R$ NO PEDIDO 📛  + 150 R$ DE FRETE GRÁTIS 💸  APROVEITEM A PROMOÇÃO.🛑 LINK NO PERFIL🛑  💸 Cupom: hrp32015999 💸  ✅+45 R$ OFF✅  🍟🍕🍔🍟🍕🍔🍟🍕🍔🍟🍕🍔🍟🍕🍔 cupom ifood  e uber eats e 99\n",
            "1340919541338148864 2020-12-21 07:18:26 +0000 <_smithsydny> @iamchika14 @Uber Before nah man hand you wan carry? 🤑🤑\n",
            "1340919536632164353 2020-12-21 07:18:25 +0000 <iwaloyeseun> @Odunadekolade Morning Sir, I really need help to fix my car immediately and started working on Uber#BebecoolChatWithQtaby\n",
            "1340919456747286528 2020-12-21 07:18:06 +0000 <marumarrun> 久しぶりにUberしたっ🥰\n",
            "1340919433091375104 2020-12-21 07:18:00 +0000 <riptidesb> hi @Uber_Support this guy is my driver and instead of bringing me to the mall he brought me to a bar. he barks he’s hot too  https://t.co/jR5Dn8HxID\n",
            "1340919429958348800 2020-12-21 07:17:59 +0000 <BenZeroUltimix> @HotChocoMilkies I would throw a Subway sandwich to give you an Uber Drive for the rest of the game. 😏😎👍🌭  https://t.co/o9z7VGk95r\n",
            "1340919363164057605 2020-12-21 07:17:43 +0000 <matheussgomesf> me amarro em uber c mt bala, amasso mt\n",
            "1340919349616336897 2020-12-21 07:17:40 +0000 <K10DFZ30E10> @tomonori_Uber 片想い。。。\n",
            "1340919289289854977 2020-12-21 07:17:26 +0000 <mltjrv> Uber driver: “where are you moving to?” Me: “Estonia, so Estland in Dutch” Me: *goes on a rant about the political situation in Estonia* Driver: *starts talking about Iceland*  I think this guy thinks now Iceland has nazis in the government :/\n",
            "1340919217269444609 2020-12-21 07:17:09 +0000 <PresidenteANEI> Los repartidores también sufren la crisis: ¿cuánto gana un ‘rider’ de Glovo, Uber Eats, Deliveroo y Just Eat?  https://t.co/kFlXHS4nzR a través de @el_pais\n",
            "1340919143113977856 2020-12-21 07:16:51 +0000 <vbEvE0AsCfbIJEg> おうちスタバ☕Uberすごい🌸これで200円くらいだった。子供と一緒だとおうちの方がカフェよりゆっくりできるしありがたい。。 こういう時間は大事だ。  #おうち時間を楽しもう  #おうちカフェ #おうちスタバ #ウーバーイーツ  #子育て  #2才 #イヤイヤ期  https://t.co/f1Xqp0pRUU\n",
            "1340919135274823680 2020-12-21 07:16:49 +0000 <ninjarinko> @warutan_Uber 生まれも育ちも西宮です！ 上ヶ原が実家です\n",
            "1340919083756302336 2020-12-21 07:16:37 +0000 <am_nzuza> @Uber why am I being charged for Uber Pass when I've never opted for that?\n",
            "1340919074956734467 2020-12-21 07:16:35 +0000 <IAmTaco> My Uber driver playing some E40 faithfully and let me run back in the spot to grab my pack, 5 stars fa sho\n",
            "1340919057781055488 2020-12-21 07:16:31 +0000 <ChaJam> essa @justwillian_ tava tão louca que esqueceu o celular dentro do uber\n",
            "1340919056468209664 2020-12-21 07:16:30 +0000 <Victor_1911_> @Sofiafobica Ya bájate del uber  https://t.co/bLYUNUt4Dz\n",
            "1340919010544680963 2020-12-21 07:16:19 +0000 <daveymadeit> Your superpower is... you can teleport anywhere except home, you gotta Uber back home.\n",
            "1340918944614535173 2020-12-21 07:16:04 +0000 <Tina_Hokwana> Uber/Bolt drivers will demand a 5 star rating kodwa umntu uqhuba ikhwapha.\n",
            "1340918918605537284 2020-12-21 07:15:57 +0000 <anforafunesta> creen q si pido por uber eats unos churrumais a las 2 AM me los traigan?\n",
            "1340918912301604864 2020-12-21 07:15:56 +0000 <nithdaguy> i asked my uber driver what typa girls he likes and then he told me that jesus is the only way\n",
            "1340918911404027904 2020-12-21 07:15:56 +0000 <eae_giv> Parça, o Uber tá caro assim pq é fim de ano ou é pira minha? Caralho eu queria ir do inamar pro shop e deu 20 conto?????\n",
            "1340918909960998912 2020-12-21 07:15:55 +0000 <ryo1umidanji> @takeshi_uber 中の人に聞いた話では今年のクリスマスはケンタ、各社デリバリー止めるらしいです。予約の店頭販売のみになるらしいです。しらんけどw\n",
            "1340918869796392961 2020-12-21 07:15:46 +0000 <tuxGxaqWGK3b4qm> 【デリバリー沼にハマり中！】「Uberイーツを皮切りに、すべてのデリバリーで配達をしたいと日々情報収集に勤しんでいます笑」 #スカパー #いい沼ハマってんね #いい沼ハマってんねグランプリ   https://t.co/RxGIoZzwFQ\n",
            "1340918867548393472 2020-12-21 07:15:45 +0000 <CulturalDead> @HyveMynd I am the uber nerd when it comes to rpgs. That being said I'm starting a couple week run of Alien for my regular group on Tuesday. Should be a lot of fun.\n",
            "1340918857863598081 2020-12-21 07:15:43 +0000 <genshin_regulus> えっ？ドラスパでもUberイベントあるの？ (    ゜д  ゜)\n",
            "1340918740683264000 2020-12-21 07:15:15 +0000 <AOtaku65> @THESLPLAYER Aw, ur pouty face is super cute Moge-ko: “Writer, ur having too much fun....LET ME JOIN IN! Starsy is super duper uber cute” Mogekov: “If my sis says so, then i agree”\n",
            "1340918722194620416 2020-12-21 07:15:11 +0000 <GlennTituss> @Uber_Support Good Evening. I just got an email about my background check on how my account is on hold. I have questions. Please DM me. Thank you!\n",
            "1340918708760440832 2020-12-21 07:15:07 +0000 <okk__gabyyy> @Eterno_Concreto @qlf_daantas ok, vai pagar meu Uber tb?\n",
            "1340918633975873539 2020-12-21 07:14:50 +0000 <EdmontonRASC> Fine pics by @kmoorephotos. One can just about resolve the rings of Saturn, &amp; all 4 moons of Jupiter can be seen, tho' only 2 easily. L-R, Callisto (uber-faint), Ganymede, Jupiter, Io, Europa (faint). Callisto is ~2x as far from Jupiter as is Ganymede;  Europa right next to Io.\n",
            "1340918602296389632 2020-12-21 07:14:42 +0000 <IamEbuka_> Went out without my card for the 600th time. Now I have to decide if I'll just take the expensive uber or go back home and pickup my card.\n",
            "1340918564920971264 2020-12-21 07:14:33 +0000 <eilishxbbh> @ilomiloei omg ya estoy en el uber\n",
            "1340918563255808001 2020-12-21 07:14:33 +0000 <RadheSingu> @ShieldVoC @Uber_Support day by day your customer support and experience is getting worst and worst. You have hired unprofessional and illiterate drivers in your company who don’t know how to deal with customers. I think your company is busy making instead of customer support.\n",
            "1340918547506057216 2020-12-21 07:14:29 +0000 <CharaiSuper> @atmonotone けーき…‼️女子力の塊❣️  ただうちにオーブンが無いんです😭そして当日まで休みなくて準備できなさそうで ご飯もUberにしようと思ってしまってま🥲  でもケーキ要りますね‼️ ありがとうございます😊\n",
            "1340918544083632129 2020-12-21 07:14:28 +0000 <ongubo_> Leo nipeane free uber rides?\n",
            "1340918533346254850 2020-12-21 07:14:26 +0000 <lifeburn78> @HCare1st @laurenboebert The uber wealthy already pay more in taxes in one year than you pay in your lifetime\n",
            "1340918530078740480 2020-12-21 07:14:25 +0000 <DeveshS07579318> @UberINSupport @Uber_India  Why you are charging 105 rupees for Toll prices and Fees despite there is no Toll was paid during my whole trip.   I need it refund Immediately ....  https://t.co/kXSOqVku0L\n",
            "1340918448965050369 2020-12-21 07:14:05 +0000 <kuldeepkeshwar> @_prateekbh @Uber congrats Prateek!!\n",
            "1340918437372149762 2020-12-21 07:14:03 +0000 <PoliticsScale> @AnnCoulter The privileged people who run this uber-posh school feel guilty b/c they're rich and they serve people who are perhaps even wealthier. Rather than face the possible social injustice of 'classism,' which would be disadvantageous to themselves, they redirect attention to 'racism.'\n",
            "1340918410478198786 2020-12-21 07:13:56 +0000 <RagnarNowak> Uber invests in Poland  https://t.co/Cx6GCu2XbN\n",
            "1340918404543205378 2020-12-21 07:13:55 +0000 <larsereth> @AnonPig That makes a lot of sense now knowing that he didn’t actually open up a bunch of shops in a global pandemic. Its a common practice among chain restaurants to sell foods on Uber Eats etc under pseudonyms. Chuck E Cheese sells pizza as Pasqually’s on food delivery apps for example\n",
            "1340918381508255744 2020-12-21 07:13:49 +0000 <leocpx1> O uber deu 150 mas ela não liga porque é o pai que banca\n",
            "1340918376458280961 2020-12-21 07:13:48 +0000 <naomi_eunki> @LaloElizondoTV @UberEats_mex @Uber @Profeco De que me perdí???\n",
            "1340918372016349185 2020-12-21 07:13:47 +0000 <AnthonyLeeTitus> @Uber_Support Hello Uber, I have a few questions about why my application for driving for uber is taking so long\n",
            "1340918299132096512 2020-12-21 07:13:30 +0000 <jooiararissimaa> Se o uber chegar fudeu ainda bem q nn pedi nd, tô podendo perder dinheiro pra ngm 🙅🏽‍♀️😌\n",
            "1340918210602917888 2020-12-21 07:13:09 +0000 <LaloElizondoTV> Son un maldito asco @UberEats_mex @Uber ojalá @Profeco multará o suspendiera esta aplicación por incumplimientos y mal trato al cliente\n",
            "1340918202214174721 2020-12-21 07:13:07 +0000 <ko_crin_sub> @mgmgmpmgd お店側は30%以上支払うからUberで配達すればするほど赤字になるw（自分ん所で配達員雇うとそれぐらいかかるらしいから決して高い方ではないらしい）  雇用を作ってくれたのは良いこと！（Uberは成果報酬、ライバルの出前館は時給制）\n",
            "1340918158526279681 2020-12-21 07:12:56 +0000 <Donny_Deal> @CambiBrown @mattty_k @JuanWeek I was telling the guys how on Murillo occasions I’ve ordered Uber eats and fell asleep....one time even ordering twice and falling asleep on both orders separately\n",
            "1340918124330295296 2020-12-21 07:12:48 +0000 <4TeXno> Бирюльки №589. Про измерение кислорода в крови и другие параметры    Почему измерение кислорода в крови неточно, как мерить давление часами; Uber в убытках; Google и Apple против COVID; AirPods Pro и тре...  https://t.co/LElScGG3t5, Смартфоны  https://t.co/mvguREKx6O  https://t.co/YQbPDVVeY8\n",
            "1340918112737259520 2020-12-21 07:12:45 +0000 <DrunkyGa> Oh my god... forgot my brand new phone inside the Uber MINUTES before boarding my train for vacation\n",
            "1340918102435885057 2020-12-21 07:12:43 +0000 <PUBG_Ryuki> uberのこのスピード感まじ有難いw  https://t.co/Hyruk42a6f\n",
            "1340918045607305217 2020-12-21 07:12:29 +0000 <ashwinchill> @Uber_Support You guys charged me Rs20 for this both times😡😡i wish consumer court in india is strong enough to file a case\n",
            "1340918024765906945 2020-12-21 07:12:24 +0000 <NotMetaphoric> I know it's possible to delegate all that. Might just fire up a game as an uber rich club in a non-competitive league and focus on the games alone and delegate every single other thing.\n",
            "1340918019577569280 2020-12-21 07:12:23 +0000 <Who_ThatPokemon> I’m cracking up in the back of this uber and I know this driver is scared 💀\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXeXUL_yFilw"
      },
      "source": [
        "### The End (of the data extraction)\n",
        "the stuff below is just some cleanup..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 796
        },
        "id": "Ronk2F_Wbqj9",
        "outputId": "acde3050-ca80-48c4-c00a-4af32520fb78"
      },
      "source": [
        "# Quick check\n",
        "twint.storage.panda.Tweets_df.head(1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>conversation_id</th>\n",
              "      <th>created_at</th>\n",
              "      <th>date</th>\n",
              "      <th>timezone</th>\n",
              "      <th>place</th>\n",
              "      <th>tweet</th>\n",
              "      <th>language</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>cashtags</th>\n",
              "      <th>user_id</th>\n",
              "      <th>user_id_str</th>\n",
              "      <th>username</th>\n",
              "      <th>name</th>\n",
              "      <th>day</th>\n",
              "      <th>hour</th>\n",
              "      <th>link</th>\n",
              "      <th>urls</th>\n",
              "      <th>photos</th>\n",
              "      <th>video</th>\n",
              "      <th>thumbnail</th>\n",
              "      <th>retweet</th>\n",
              "      <th>nlikes</th>\n",
              "      <th>nreplies</th>\n",
              "      <th>nretweets</th>\n",
              "      <th>quote_url</th>\n",
              "      <th>search</th>\n",
              "      <th>near</th>\n",
              "      <th>geo</th>\n",
              "      <th>source</th>\n",
              "      <th>user_rt_id</th>\n",
              "      <th>user_rt</th>\n",
              "      <th>retweet_id</th>\n",
              "      <th>reply_to</th>\n",
              "      <th>retweet_date</th>\n",
              "      <th>translate</th>\n",
              "      <th>trans_src</th>\n",
              "      <th>trans_dest</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1340920678481063937</td>\n",
              "      <td>1340906025126268930</td>\n",
              "      <td>1.608535e+12</td>\n",
              "      <td>2020-12-21 07:22:57</td>\n",
              "      <td>+0000</td>\n",
              "      <td></td>\n",
              "      <td>@SGoyal3189 Authorization holds are voided by ...</td>\n",
              "      <td>en</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>794125504357900289</td>\n",
              "      <td>794125504357900289</td>\n",
              "      <td>UberINSupport</td>\n",
              "      <td>Uber India Support</td>\n",
              "      <td>1</td>\n",
              "      <td>07</td>\n",
              "      <td>https://twitter.com/UberINSupport/status/13409...</td>\n",
              "      <td>[https://twitter.com/messages/compose?recipien...</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>uber</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>[{'screen_name': 'SGoyal3189', 'name': 'Sourab...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1340920675930759168</td>\n",
              "      <td>1340920675930759168</td>\n",
              "      <td>1.608535e+12</td>\n",
              "      <td>2020-12-21 07:22:56</td>\n",
              "      <td>+0000</td>\n",
              "      <td></td>\n",
              "      <td>Uber vende su unidad de vehículos autónomos po...</td>\n",
              "      <td>es</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>448706845</td>\n",
              "      <td>448706845</td>\n",
              "      <td>EuskalValley</td>\n",
              "      <td>EuskalValley</td>\n",
              "      <td>1</td>\n",
              "      <td>07</td>\n",
              "      <td>https://twitter.com/EuskalValley/status/134092...</td>\n",
              "      <td>[https://cincodias.elpais.com/cincodias/2020/1...</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>uber</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1340920625515307010</td>\n",
              "      <td>1340920625515307010</td>\n",
              "      <td>1.608535e+12</td>\n",
              "      <td>2020-12-21 07:22:44</td>\n",
              "      <td>+0000</td>\n",
              "      <td></td>\n",
              "      <td>A mate sent me this screenshot from a Sydney U...</td>\n",
              "      <td>en</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>24352918</td>\n",
              "      <td>24352918</td>\n",
              "      <td>meghnabali</td>\n",
              "      <td>Meghna Bali मेघना बाली</td>\n",
              "      <td>1</td>\n",
              "      <td>07</td>\n",
              "      <td>https://twitter.com/meghnabali/status/13409206...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[https://pbs.twimg.com/media/EpvoMlyU0AICiG6.jpg]</td>\n",
              "      <td>1</td>\n",
              "      <td>https://pbs.twimg.com/media/EpvoMlyU0AICiG6.jpg</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>uber</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1340920604363350016</td>\n",
              "      <td>1340920604363350016</td>\n",
              "      <td>1.608535e+12</td>\n",
              "      <td>2020-12-21 07:22:39</td>\n",
              "      <td>+0000</td>\n",
              "      <td></td>\n",
              "      <td>hey all my followrrs!! got free uber eats code...</td>\n",
              "      <td>en</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>1332534952559316994</td>\n",
              "      <td>1332534952559316994</td>\n",
              "      <td>SoftcoreV</td>\n",
              "      <td>Virgin Suicide</td>\n",
              "      <td>1</td>\n",
              "      <td>07</td>\n",
              "      <td>https://twitter.com/SoftcoreV/status/134092060...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>uber</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1340920584247664641</td>\n",
              "      <td>1340920584247664641</td>\n",
              "      <td>1.608535e+12</td>\n",
              "      <td>2020-12-21 07:22:35</td>\n",
              "      <td>+0000</td>\n",
              "      <td></td>\n",
              "      <td>HAHAHA JUAL KASUT BAYAR UBER LEH TAK</td>\n",
              "      <td>in</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>1031165912508846081</td>\n",
              "      <td>1031165912508846081</td>\n",
              "      <td>_mmiaaax</td>\n",
              "      <td>Mia</td>\n",
              "      <td>1</td>\n",
              "      <td>07</td>\n",
              "      <td>https://twitter.com/_mmiaaax/status/1340920584...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>uber</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>1340918112737259520</td>\n",
              "      <td>1340918112737259520</td>\n",
              "      <td>1.608535e+12</td>\n",
              "      <td>2020-12-21 07:12:45</td>\n",
              "      <td>+0000</td>\n",
              "      <td></td>\n",
              "      <td>Oh my god... forgot my brand new phone inside ...</td>\n",
              "      <td>en</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>1654355263</td>\n",
              "      <td>1654355263</td>\n",
              "      <td>DrunkyGa</td>\n",
              "      <td>JP✨</td>\n",
              "      <td>1</td>\n",
              "      <td>07</td>\n",
              "      <td>https://twitter.com/DrunkyGa/status/1340918112...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>uber</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>1340918102435885057</td>\n",
              "      <td>1340918102435885057</td>\n",
              "      <td>1.608535e+12</td>\n",
              "      <td>2020-12-21 07:12:43</td>\n",
              "      <td>+0000</td>\n",
              "      <td></td>\n",
              "      <td>uberのこのスピード感まじ有難いw  https://t.co/Hyruk42a6f</td>\n",
              "      <td>ja</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>3309190779</td>\n",
              "      <td>3309190779</td>\n",
              "      <td>PUBG_Ryuki</td>\n",
              "      <td>S͜QEx٭ Ryuki</td>\n",
              "      <td>1</td>\n",
              "      <td>07</td>\n",
              "      <td>https://twitter.com/PUBG_Ryuki/status/13409181...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[https://pbs.twimg.com/media/Epvl5-OUcAAFGaq.jpg]</td>\n",
              "      <td>1</td>\n",
              "      <td>https://pbs.twimg.com/media/Epvl5-OUcAAFGaq.jpg</td>\n",
              "      <td>False</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>uber</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>1340918045607305217</td>\n",
              "      <td>1340501235707727872</td>\n",
              "      <td>1.608535e+12</td>\n",
              "      <td>2020-12-21 07:12:29</td>\n",
              "      <td>+0000</td>\n",
              "      <td></td>\n",
              "      <td>@Uber_Support You guys charged me Rs20 for thi...</td>\n",
              "      <td>en</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>197084518</td>\n",
              "      <td>197084518</td>\n",
              "      <td>ashwinchill</td>\n",
              "      <td>ashwin chilappagari</td>\n",
              "      <td>1</td>\n",
              "      <td>07</td>\n",
              "      <td>https://twitter.com/ashwinchill/status/1340918...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>uber</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>[{'screen_name': 'Uber_Support', 'name': 'Uber...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>1340918024765906945</td>\n",
              "      <td>1340916514686418945</td>\n",
              "      <td>1.608535e+12</td>\n",
              "      <td>2020-12-21 07:12:24</td>\n",
              "      <td>+0000</td>\n",
              "      <td></td>\n",
              "      <td>I know it's possible to delegate all that. Mig...</td>\n",
              "      <td>en</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>1079084050546114560</td>\n",
              "      <td>1079084050546114560</td>\n",
              "      <td>NotMetaphoric</td>\n",
              "      <td>Weirdo in the Worm Store</td>\n",
              "      <td>1</td>\n",
              "      <td>07</td>\n",
              "      <td>https://twitter.com/NotMetaphoric/status/13409...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>uber</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>1340918019577569280</td>\n",
              "      <td>1340918019577569280</td>\n",
              "      <td>1.608535e+12</td>\n",
              "      <td>2020-12-21 07:12:23</td>\n",
              "      <td>+0000</td>\n",
              "      <td></td>\n",
              "      <td>I’m cracking up in the back of this uber and I...</td>\n",
              "      <td>en</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>1140673064671358976</td>\n",
              "      <td>1140673064671358976</td>\n",
              "      <td>Who_ThatPokemon</td>\n",
              "      <td>Chikachu</td>\n",
              "      <td>1</td>\n",
              "      <td>07</td>\n",
              "      <td>https://twitter.com/Who_ThatPokemon/status/134...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>uber</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>98 rows × 38 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                     id      conversation_id  ...  trans_src trans_dest\n",
              "0   1340920678481063937  1340906025126268930  ...                      \n",
              "1   1340920675930759168  1340920675930759168  ...                      \n",
              "2   1340920625515307010  1340920625515307010  ...                      \n",
              "3   1340920604363350016  1340920604363350016  ...                      \n",
              "4   1340920584247664641  1340920584247664641  ...                      \n",
              "..                  ...                  ...  ...        ...        ...\n",
              "93  1340918112737259520  1340918112737259520  ...                      \n",
              "94  1340918102435885057  1340918102435885057  ...                      \n",
              "95  1340918045607305217  1340501235707727872  ...                      \n",
              "96  1340918024765906945  1340916514686418945  ...                      \n",
              "97  1340918019577569280  1340918019577569280  ...                      \n",
              "\n",
              "[98 rows x 38 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWmqmKbbiRx6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "7b589f39-ad0e-4989-da56-4280c48bce96"
      },
      "source": [
        "# Cleanup\n",
        "\n",
        "Tweets_df.drop(Tweets_df.columns[[0,1,2,4,5,7,8,9,10,11,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37]], axis = 1, inplace = True) \n",
        "#tweets = tweets([\"date\", \"username\", \"tweet\", \"hashtags\", \"nlikes\"])\n",
        "Tweets_df.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>tweet</th>\n",
              "      <th>username</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-12-21 07:21:25</td>\n",
              "      <td>@jimkat2 The same (mostly terrible) drivers of...</td>\n",
              "      <td>ryancrawcour</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-12-21 07:21:20</td>\n",
              "      <td>When you’re faded and trying to see if it’s yo...</td>\n",
              "      <td>YoungestBode</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-12-21 07:21:16</td>\n",
              "      <td>@BloodyPeachh We definitely want to get this s...</td>\n",
              "      <td>Uber_Support</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-12-21 07:21:13</td>\n",
              "      <td>I really don't understand why my laid back ass...</td>\n",
              "      <td>barelyhermes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-12-21 07:21:10</td>\n",
              "      <td>今日Uberのサイト見てて、カートに商品入れて会計の画面で評価（チップ）の画面が出てきました...</td>\n",
              "      <td>saesaenosae</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2020-12-21 07:21:07</td>\n",
              "      <td>@UberEats yo Uber wtf is going on me and my be...</td>\n",
              "      <td>itsangiehehe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2020-12-21 07:21:05</td>\n",
              "      <td>@AIhXeeSgaNHxqM6 騙された〜🤣時間2なら全然良いじゃないですかw  DiDi...</td>\n",
              "      <td>rei_roku</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2020-12-21 07:21:02</td>\n",
              "      <td>@Renefabian6 Es que te mamas también jajajajaj...</td>\n",
              "      <td>karinaamoreno</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2020-12-21 07:20:57</td>\n",
              "      <td>Uber Eats 😍😍😍😍</td>\n",
              "      <td>SirAlexas</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2020-12-21 07:20:55</td>\n",
              "      <td>@TheFeedSBS @SBSNews What an idiot. Obviously ...</td>\n",
              "      <td>WardenAJames</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  date  ...       username\n",
              "0  2020-12-21 07:21:25  ...   ryancrawcour\n",
              "1  2020-12-21 07:21:20  ...   YoungestBode\n",
              "2  2020-12-21 07:21:16  ...   Uber_Support\n",
              "3  2020-12-21 07:21:13  ...   barelyhermes\n",
              "4  2020-12-21 07:21:10  ...    saesaenosae\n",
              "5  2020-12-21 07:21:07  ...   itsangiehehe\n",
              "6  2020-12-21 07:21:05  ...       rei_roku\n",
              "7  2020-12-21 07:21:02  ...  karinaamoreno\n",
              "8  2020-12-21 07:20:57  ...      SirAlexas\n",
              "9  2020-12-21 07:20:55  ...   WardenAJames\n",
              "\n",
              "[10 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1VSc-YxknvE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75630095-0587-44ae-b22a-76f3e82bc7ec"
      },
      "source": [
        "# Done\n",
        "from pyspark.sql import SparkSession\n",
        "from optimus import Optimus\n",
        "spark = SparkSession.builder.appName('optimus').getOrCreate()\n",
        "op= Optimus(spark)\n",
        "\n",
        "op = Optimus(verbose=True)\n",
        "pdf=Tweets_df\n",
        "\n",
        "new = op.create.df(pdf= pdf)\n",
        "\n",
        "new\\\n",
        "    .cols.date_transform(\"date\", \"yyyy/MM/dd\", \"dd-MM-YYYY\")\\\n",
        "    .cols.remove_accents(\"tweet\")\\\n",
        "    .cols.remove_special_chars(\"tweet\")\\\n",
        "    .cols.trim(\"*\")\\\n",
        "\n",
        "new.table()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n",
            "\n",
            "    You are using PySparkling of version 2.4.10, but your PySpark is of\n",
            "    version 3.0.1. Please make sure Spark and PySparkling versions are compatible. \n",
            "INFO:optimus:Operative System:Linux\n",
            "INFO:optimus:Just check that Spark and all necessary environments vars are present...\n",
            "INFO:optimus:-----\n",
            "INFO:optimus:SPARK_HOME=/content/spark-3.0.1-bin-hadoop3.2\n",
            "INFO:optimus:HADOOP_HOME is not set\n",
            "INFO:optimus:PYSPARK_PYTHON=/usr/bin/python3\n",
            "INFO:optimus:PYSPARK_DRIVER_PYTHON is not set\n",
            "INFO:optimus:PYSPARK_SUBMIT_ARGS=--jars \"file:///usr/local/lib/python3.6/dist-packages/optimus/jars/spark-redis-2.4.1-SNAPSHOT-jar-with-dependencies.jar,file:///usr/local/lib/python3.6/dist-packages/optimus/jars/RedshiftJDBC42-1.2.16.1027.jar,file:///usr/local/lib/python3.6/dist-packages/optimus/jars/mysql-connector-java-8.0.16.jar,file:///usr/local/lib/python3.6/dist-packages/optimus/jars/ojdbc8.jar,file:///usr/local/lib/python3.6/dist-packages/optimus/jars/postgresql-42.2.5.jar,file:///usr/local/lib/python3.6/dist-packages/optimus/jars/presto-jdbc-0.224.jar,file:///usr/local/lib/python3.6/dist-packages/optimus/jars/spark-cassandra-connector_2.11-2.4.1.jar,file:///usr/local/lib/python3.6/dist-packages/optimus/jars/sqlite-jdbc-3.27.2.1.jar,file:///usr/local/lib/python3.6/dist-packages/optimus/jars/mssql-jdbc-7.4.1.jre8.jar\" --driver-class-path \"/usr/local/lib/python3.6/dist-packages/optimus/jars/spark-redis-2.4.1-SNAPSHOT-jar-with-dependencies.jar:/usr/local/lib/python3.6/dist-packages/optimus/jars/RedshiftJDBC42-1.2.16.1027.jar:/usr/local/lib/python3.6/dist-packages/optimus/jars/mysql-connector-java-8.0.16.jar:/usr/local/lib/python3.6/dist-packages/optimus/jars/ojdbc8.jar:/usr/local/lib/python3.6/dist-packages/optimus/jars/postgresql-42.2.5.jar:/usr/local/lib/python3.6/dist-packages/optimus/jars/presto-jdbc-0.224.jar:/usr/local/lib/python3.6/dist-packages/optimus/jars/spark-cassandra-connector_2.11-2.4.1.jar:/usr/local/lib/python3.6/dist-packages/optimus/jars/sqlite-jdbc-3.27.2.1.jar:/usr/local/lib/python3.6/dist-packages/optimus/jars/mssql-jdbc-7.4.1.jre8.jar\" --conf \"spark.sql.catalogImplementation=hive\" pyspark-shell\n",
            "INFO:optimus:JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n",
            "INFO:optimus:Pyarrow Installed\n",
            "INFO:optimus:-----\n",
            "INFO:optimus:Starting or getting SparkSession and SparkContext...\n",
            "INFO:optimus:Spark Version:3.0.1\n",
            "INFO:optimus:\n",
            "                             ____        __  _                     \n",
            "                            / __ \\____  / /_(_)___ ___  __  _______\n",
            "                           / / / / __ \\/ __/ / __ `__ \\/ / / / ___/\n",
            "                          / /_/ / /_/ / /_/ / / / / / / /_/ (__  ) \n",
            "                          \\____/ .___/\\__/_/_/ /_/ /_/\\__,_/____/  \n",
            "                              /_/                                  \n",
            "                              \n",
            "INFO:optimus:Transform and Roll out...\n",
            "INFO:optimus:Optimus successfully imported. Have fun :).\n",
            "INFO:optimus:Config.ini not found\n",
            "INFO:optimus:Using 'column_exp' to process column 'date' with function _date_transform\n",
            "INFO:optimus:Using 'pandas_udf' to process column 'tweet' with function _remove_accents\n",
            "INFO:optimus:Using 'pandas_udf' to process column 'tweet' with function multiple_replace\n",
            "INFO:optimus:Using 'column_exp' to process column 'date' with function _trim\n",
            "INFO:optimus:Using 'column_exp' to process column 'tweet' with function _trim\n",
            "INFO:optimus:Using 'column_exp' to process column 'username' with function _trim\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-------------------+---------------------------------+-------------+\n",
            "|               date|                            tweet|     username|\n",
            "+-------------------+---------------------------------+-------------+\n",
            "|2020-12-21 07:21:25|             @jimkat2 The same...| ryancrawcour|\n",
            "|2020-12-21 07:21:20|             When you’re faded...| YoungestBode|\n",
            "|2020-12-21 07:21:16|             @BloodyPeachh We ...| Uber_Support|\n",
            "|2020-12-21 07:21:13|             I really don't un...| barelyhermes|\n",
            "|2020-12-21 07:21:10|今日Uberのサイト見てて、カート...|  saesaenosae|\n",
            "|2020-12-21 07:21:07|             @UberEats yo Uber...| itsangiehehe|\n",
            "|2020-12-21 07:21:05|             @AIhXeeSgaNHxqM6 ...|     rei_roku|\n",
            "|2020-12-21 07:21:02|             @Renefabian6 Es q...|karinaamoreno|\n",
            "|2020-12-21 07:20:57|               Uber Eats 😍😍😍😍|    SirAlexas|\n",
            "|2020-12-21 07:20:55|             @TheFeedSBS @SBSN...| WardenAJames|\n",
            "|2020-12-21 07:20:54|             I’m going to orde...|     BietjieG|\n",
            "|2020-12-21 07:20:53|             Fui enganada pela...|thaaliamartin|\n",
            "|2020-12-21 07:20:42|             eu não paguei 21,...| oliveirxglau|\n",
            "|2020-12-21 07:20:31|         @Uber_SAGAMI 他人のは...|  eat33930535|\n",
            "|2020-12-21 07:20:23|             eu sou completame...|     aluizax_|\n",
            "|2020-12-21 07:20:23|             @ryo1umidanji @ta...| orangina_san|\n",
            "|2020-12-21 07:20:21|             @coryodaniel @Mat...|christhekeele|\n",
            "|2020-12-21 07:20:16|             @riptidesb @Uber_...|   3O5HOLLAND|\n",
            "|2020-12-21 07:20:09|             miss telling my b...|   sharamamas|\n",
            "|2020-12-21 07:20:04|             @UBER_eats_ramen ...| oyu_oyu_sayu|\n",
            "+-------------------+---------------------------------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Haa746_tFfe2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "753d35ae-a9e9-4864-9e93-7d1b32b1722e"
      },
      "source": [
        "\n",
        "#new.filter(new.tweet.rlike(\"modi | .*scanner(.*)\")).show() \n",
        "b=str(input('Sort on:::::?'))\n",
        "df=new.filter(new.tweet.like(\"%b%\"))\n",
        "#ew.filter(new.tweet.like(\"% sensor %\")).show()\n",
        "#new.filter(new.tweet.like(\"oneplus %\")).show()\n",
        "#new.filter(new.tweet.like(\"% Google Pixel 5 %\")).show()\n",
        "#new.filter(new.tweet.like(\"% food %\")).show()\n",
        "#new.filter(new.tweet.like(\"% garbage %\")).show()\n",
        "#new.filter(new.select(\"tweet\", \"username\",new.tweet.like(\"% sensor size %\")).show())\n",
        "# For multiple keywords, extract using like function and extract into csv then again append the results or use rlike (doubt on which re is effective!!)\n",
        "#new.save.csv(\"modi.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sort on:::::?driver\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxTK3-wWpr4I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d837f2d2-c99f-41c8-b256-8be2b6a2ee4b"
      },
      "source": [
        "type(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.dataframe.DataFrame"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLD3BT1zc930",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "83d58371-67d1-44cb-8993-2aa32565cb79"
      },
      "source": [
        "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
        "result_pdf = df.select(\"*\").toPandas()\n",
        "result_pdf[\"Searched Keyword\"]=a\n",
        "result_pdf[\"Sort on Keyword\"]=b\n",
        "result_pdf\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>tweet</th>\n",
              "      <th>username</th>\n",
              "      <th>Searched Keyword</th>\n",
              "      <th>Sort on Keyword</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-12-21 07:21:25</td>\n",
              "      <td>@jimkat2 The same (mostly terrible) drivers of...</td>\n",
              "      <td>ryancrawcour</td>\n",
              "      <td>uber</td>\n",
              "      <td>driver</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-12-21 07:21:20</td>\n",
              "      <td>When you’re faded and trying to see if it’s yo...</td>\n",
              "      <td>YoungestBode</td>\n",
              "      <td>uber</td>\n",
              "      <td>driver</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-12-21 07:21:16</td>\n",
              "      <td>@BloodyPeachh We definitely want to get this s...</td>\n",
              "      <td>Uber_Support</td>\n",
              "      <td>uber</td>\n",
              "      <td>driver</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-12-21 07:21:13</td>\n",
              "      <td>I really don't understand why my laid back ass...</td>\n",
              "      <td>barelyhermes</td>\n",
              "      <td>uber</td>\n",
              "      <td>driver</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-12-21 07:21:10</td>\n",
              "      <td>今日Uberのサイト見てて、カートに商品入れて会計の画面で評価（チップ）の画面が出てきました...</td>\n",
              "      <td>saesaenosae</td>\n",
              "      <td>uber</td>\n",
              "      <td>driver</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>2020-12-21 07:11:14</td>\n",
              "      <td>Opinion | Uber, Instacart and others’ vaccine ...</td>\n",
              "      <td>JamesTobyne</td>\n",
              "      <td>uber</td>\n",
              "      <td>driver</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>2020-12-21 07:11:00</td>\n",
              "      <td>@kogitaro_uber @ubaosandesu パンツがカレーパンって香辛料でしみな...</td>\n",
              "      <td>kikokicoki_ko</td>\n",
              "      <td>uber</td>\n",
              "      <td>driver</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>2020-12-21 07:10:55</td>\n",
              "      <td>@Uber_KSA لو سمحتو سيارتي ٢٠١٣؟(اقدر اشتغل عليها</td>\n",
              "      <td>A1657825024</td>\n",
              "      <td>uber</td>\n",
              "      <td>driver</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>2020-12-21 07:10:31</td>\n",
              "      <td>寒〜い冬は、ウーバーイーツでぬくぬくおうちご飯⛄️❄️ Uber Eats初回限定クーポン✨...</td>\n",
              "      <td>uberwaribiki</td>\n",
              "      <td>uber</td>\n",
              "      <td>driver</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>2020-12-21 07:10:28</td>\n",
              "      <td>lmao not the Uber driver watching me shift my ...</td>\n",
              "      <td>naomiamorr</td>\n",
              "      <td>uber</td>\n",
              "      <td>driver</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>93 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                   date  ... Sort on Keyword\n",
              "0   2020-12-21 07:21:25  ...          driver\n",
              "1   2020-12-21 07:21:20  ...          driver\n",
              "2   2020-12-21 07:21:16  ...          driver\n",
              "3   2020-12-21 07:21:13  ...          driver\n",
              "4   2020-12-21 07:21:10  ...          driver\n",
              "..                  ...  ...             ...\n",
              "88  2020-12-21 07:11:14  ...          driver\n",
              "89  2020-12-21 07:11:00  ...          driver\n",
              "90  2020-12-21 07:10:55  ...          driver\n",
              "91  2020-12-21 07:10:31  ...          driver\n",
              "92  2020-12-21 07:10:28  ...          driver\n",
              "\n",
              "[93 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TK8qsbNWdBvL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "1b12b4f6-b907-4561-fe09-19c96e5394e1"
      },
      "source": [
        "result_pdf.to_csv(index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'date,tweet,username,Searched Keyword,Sort on Keyword\\n2020-12-21 07:21:25,@jimkat2 The same (mostly terrible) drivers often drive (the same mostly terrible  cars) for multiple services. I miss US Uber.  Probably only miss that and Amazon though.,ryancrawcour,uber,driver\\n2020-12-21 07:21:20,When you’re faded and trying to see if it’s your Uber,YoungestBode,uber,driver\\n2020-12-21 07:21:16,\"@BloodyPeachh We definitely want to get this sorted out. Please send us a DM with the phone number that is associated with your Uber account, so we can further assist.\",Uber_Support,uber,driver\\n2020-12-21 07:21:13,I really don\\'t understand why my laid back ass decided to go out yesterday and almost curfew hour  I ended up paying 1190 for an Uber. I\\'m still stressed,barelyhermes,uber,driver\\n2020-12-21 07:21:10,今日Uberのサイト見てて、カートに商品入れて会計の画面で評価（チップ）の画面が出てきました！！クレジットを登録したからかな？,saesaenosae,uber,driver\\n2020-12-21 07:21:07,@UberEats yo Uber wtf is going on me and my bestie are hungry and your app is not working😭,itsangiehehe,uber,driver\\n2020-12-21 07:21:05,@AIhXeeSgaNHxqM6 騙された〜🤣時間2なら全然良いじゃないですかw  DiDiのインセは20件ごとに1000円とか金額が増えていくのは良いですね。これならイケる！って気になりそう。 Uberと比べて50円くらい高い？🤔,rei_roku,uber,driver\\n2020-12-21 07:21:02,@Renefabian6 Es que te mamas también jajajajajaja. Yo quiero \\U0001f97a mándalos en un Uber express JAJAJA,karinaamoreno,uber,driver\\n2020-12-21 07:20:57,Uber Eats 😍😍😍😍,SirAlexas,uber,driver\\n2020-12-21 07:20:55,@TheFeedSBS @SBSNews What an idiot. Obviously if the guy to Australia by boat he wouldn\\'t be driving an Uber. He\\'d be locked in offshore detention.,WardenAJames,uber,driver\\n2020-12-21 07:20:54,I’m going to order Uber Eats for Christmas 💔,BietjieG,uber,driver\\n2020-12-21 07:20:53,\"Fui enganada pela @Uber que me deu 2 meses de @telecine gratis, mas fui cadastrar e me cobrou 🤡🤡🤡🤡🤡\",thaaliamartin,uber,driver\\n2020-12-21 07:20:42,\"eu não paguei 21,00 de uber do iguaçu ate o caravelas não né\",oliveirxglau,uber,driver\\n2020-12-21 07:20:31,@Uber_SAGAMI 他人のは被りたくないですよね\\U0001f92e 売れるんですかね･･･,eat33930535,uber,driver\\n2020-12-21 07:20:23,\"eu sou completamente derretida pelo bjorn, uber e ivar\",aluizax_,uber,driver\\n2020-12-21 07:20:23,@ryo1umidanji @takeshi_uber  https://t.co/249mKBI148 下の方に24.25は店内飲食禁止にするとの表記が出てました😳💦,orangina_san,uber,driver\\n2020-12-21 07:20:21,\"@coryodaniel @MattOswaltVA Man goes to doctor, says \"\"I\\'m so depressed about the $600 stimulus. I may starve to death!\"\"  Doctor says, \"\"Use it to call Uber Eats!\"\"  \"\"But doctor, I AM the Uber Eats!\"\"  ~scene~\",christhekeele,uber,driver\\n2020-12-21 07:20:16,@riptidesb @Uber_Support BIE AKSMDLD,3O5HOLLAND,uber,driver\\n2020-12-21 07:20:09,miss telling my bitches the uber here😩,sharamamas,uber,driver\\n2020-12-21 07:19:45,あなたも #UberEats ( #ウーバーイーツ )で働いてみませんか❓Uber Eatsは2016年にスタートしたフードデリバリーサービスです！  https://t.co/qv0G7ZvIqO,kshkr,uber,driver\\n2020-12-21 07:19:33,2020年6月更新！【Uber Eats】ボーナス貰える招待コード  eats-fakebv  これの入力してみてください！ ##ウーバーイーツクーポン,tibi_ponta,uber,driver\\n2020-12-21 07:19:33,@Uber_leeeeee 最近、新橋から、豊洲も可能性ありますよ😭  https://t.co/1OHU7tfPET,UberEatsJPNTYO,uber,driver\\n2020-12-21 07:19:14,@DavidBrianJoll1 EU can’t do deals with private companies. Here in UK Uber has vanished everywhere - only left in London now. And even there there drivers are quitting by 600 - 800 a week. Be none left in 12 months.,AppFraud,uber,driver\\n2020-12-21 07:19:00,4 da manhã na rua e o Uber há 20 min kkkkkkkk,costinhaz1k4,uber,driver\\n2020-12-21 07:18:55,\"@theobserver83 Here to help. Please send us a DM with the mobile number associated with your Uber account as well as any information regarding this promotion like the promo code or a screenshot of the promo offer you received via email or in-app, so that we can assist you.\",Uber_Support,uber,driver\\n2020-12-21 07:18:52,一回ぐらいUber頼みたいからアプリ落としたけど頼めるお店少なくない？ほぼチェーン店だけどww 田舎ww,xx10xx01xx,uber,driver\\n2020-12-21 07:18:33,a única coisa boa é as vezes o uber deixar mais barato a corrida 😔😔😔,krlkimvtc,uber,driver\\n2020-12-21 07:18:28,@BUeOBk0nWjQrVJe @gilmaraemmanu 🍟🍕🍔🍟🍕🍔🍟🍕🍔🍟🍕🍔🍟🍕🍔  Cupom Rappi !    💸45 R$ NO PEDIDO 📛  + 150 R$ DE FRETE GRÁTIS 💸  APROVEITEM A PROMOÇÃO.🛑 LINK NO PERFIL🛑  💸 Cupom: hrp32015999 💸  ✅+45 R$ OFF✅  🍟🍕🍔🍟🍕🍔🍟🍕🍔🍟🍕🍔🍟🍕🍔 cupom ifood  e uber eats e 99,cupomrappi010,uber,driver\\n2020-12-21 07:18:26,@iamchika14 @Uber Before nah man hand you wan carry? 🤑🤑,_smithsydny,uber,driver\\n2020-12-21 07:18:25,\"@Odunadekolade Morning Sir, I really need help to fix my car immediately and started working on Uber#BebecoolChatWithQtaby\",iwaloyeseun,uber,driver\\n2020-12-21 07:18:06,久しぶりにUberしたっ\\U0001f970,marumarrun,uber,driver\\n2020-12-21 07:18:00,hi @Uber_Support this guy is my driver and instead of bringing me to the mall he brought me to a bar. he barks he’s hot too  https://t.co/jR5Dn8HxID,riptidesb,uber,driver\\n2020-12-21 07:17:59,@HotChocoMilkies I would throw a Subway sandwich to give you an Uber Drive for the rest of the game. 😏😎👍🌭  https://t.co/o9z7VGk95r,BenZeroUltimix,uber,driver\\n2020-12-21 07:17:43,\"me amarro em uber c mt bala, amasso mt\",matheussgomesf,uber,driver\\n2020-12-21 07:17:40,@tomonori_Uber 片想い。。。,K10DFZ30E10,uber,driver\\n2020-12-21 07:17:26,\"Uber driver: “where are you moving to?” Me: “Estonia, so Estland in Dutch” Me: *goes on a rant about the political situation in Estonia* Driver: *starts talking about Iceland*  I think this guy thinks now Iceland has nazis in the government :/\",mltjrv,uber,driver\\n2020-12-21 07:17:09,\"Los repartidores también sufren la crisis: ¿cuánto gana un ‘rider’ de Glovo, Uber Eats, Deliveroo y Just Eat?  https://t.co/kFlXHS4nzR a través de @el_pais\",PresidenteANEI,uber,driver\\n2020-12-21 07:16:51,おうちスタバ☕Uberすごい🌸これで200円くらいだった。子供と一緒だとおうちの方がカフェよりゆっくりできるしありがたい。。 こういう時間は大事だ。  #おうち時間を楽しもう  #おうちカフェ #おうちスタバ #ウーバーイーツ  #子育て  #2才 #イヤイヤ期  https://t.co/f1Xqp0pRUU,vbEvE0AsCfbIJEg,uber,driver\\n2020-12-21 07:16:49,@warutan_Uber 生まれも育ちも西宮です！ 上ヶ原が実家です,ninjarinko,uber,driver\\n2020-12-21 07:16:37,@Uber why am I being charged for Uber Pass when I\\'ve never opted for that?,am_nzuza,uber,driver\\n2020-12-21 07:16:35,\"My Uber driver playing some E40 faithfully and let me run back in the spot to grab my pack, 5 stars fa sho\",IAmTaco,uber,driver\\n2020-12-21 07:16:31,essa @justwillian_ tava tão louca que esqueceu o celular dentro do uber,ChaJam,uber,driver\\n2020-12-21 07:16:30,@Sofiafobica Ya bájate del uber  https://t.co/bLYUNUt4Dz,Victor_1911_,uber,driver\\n2020-12-21 07:16:19,\"Your superpower is... you can teleport anywhere except home, you gotta Uber back home.\",daveymadeit,uber,driver\\n2020-12-21 07:16:04,Uber/Bolt drivers will demand a 5 star rating kodwa umntu uqhuba ikhwapha.,Tina_Hokwana,uber,driver\\n2020-12-21 07:15:57,creen q si pido por uber eats unos churrumais a las 2 AM me los traigan?,anforafunesta,uber,driver\\n2020-12-21 07:15:56,i asked my uber driver what typa girls he likes and then he told me that jesus is the only way,nithdaguy,uber,driver\\n2020-12-21 07:15:56,\"Parça, o Uber tá caro assim pq é fim de ano ou é pira minha? Caralho eu queria ir do inamar pro shop e deu 20 conto?????\",eae_giv,uber,driver\\n2020-12-21 07:15:55,@takeshi_uber 中の人に聞いた話では今年のクリスマスはケンタ、各社デリバリー止めるらしいです。予約の店頭販売のみになるらしいです。しらんけどw,ryo1umidanji,uber,driver\\n2020-12-21 07:15:46,【デリバリー沼にハマり中！】「Uberイーツを皮切りに、すべてのデリバリーで配達をしたいと日々情報収集に勤しんでいます笑」 #スカパー #いい沼ハマってんね #いい沼ハマってんねグランプリ   https://t.co/RxGIoZzwFQ,tuxGxaqWGK3b4qm,uber,driver\\n2020-12-21 07:15:45,@HyveMynd I am the uber nerd when it comes to rpgs. That being said I\\'m starting a couple week run of Alien for my regular group on Tuesday. Should be a lot of fun.,CulturalDead,uber,driver\\n2020-12-21 07:15:43,えっ？ドラスパでもUberイベントあるの？ (    ゜д  ゜),genshin_regulus,uber,driver\\n2020-12-21 07:15:15,\"@THESLPLAYER Aw, ur pouty face is super cute Moge-ko: “Writer, ur having too much fun....LET ME JOIN IN! Starsy is super duper uber cute” Mogekov: “If my sis says so, then i agree”\",AOtaku65,uber,driver\\n2020-12-21 07:15:11,@Uber_Support Good Evening. I just got an email about my background check on how my account is on hold. I have questions. Please DM me. Thank you!,GlennTituss,uber,driver\\n2020-12-21 07:15:07,\"@Eterno_Concreto @qlf_daantas ok, vai pagar meu Uber tb?\",okk__gabyyy,uber,driver\\n2020-12-21 07:14:50,\"Fine pics by @kmoorephotos. One can just about resolve the rings of Saturn, &amp; all 4 moons of Jupiter can be seen, tho\\' only 2 easily. L-R, Callisto (uber-faint), Ganymede, Jupiter, Io, Europa (faint). Callisto is ~2x as far from Jupiter as is Ganymede;  Europa right next to Io.\",EdmontonRASC,uber,driver\\n2020-12-21 07:14:42,Went out without my card for the 600th time. Now I have to decide if I\\'ll just take the expensive uber or go back home and pickup my card.,IamEbuka_,uber,driver\\n2020-12-21 07:14:33,@ilomiloei omg ya estoy en el uber,eilishxbbh,uber,driver\\n2020-12-21 07:14:33,@ShieldVoC @Uber_Support day by day your customer support and experience is getting worst and worst. You have hired unprofessional and illiterate drivers in your company who don’t know how to deal with customers. I think your company is busy making instead of customer support.,RadheSingu,uber,driver\\n2020-12-21 07:14:29,@atmonotone けーき…‼️女子力の塊❣️  ただうちにオーブンが無いんです😭そして当日まで休みなくて準備できなさそうで ご飯もUberにしようと思ってしまってま\\U0001f972  でもケーキ要りますね‼️ ありがとうございます😊,CharaiSuper,uber,driver\\n2020-12-21 07:14:28,Leo nipeane free uber rides?,ongubo_,uber,driver\\n2020-12-21 07:14:26,@HCare1st @laurenboebert The uber wealthy already pay more in taxes in one year than you pay in your lifetime,lifeburn78,uber,driver\\n2020-12-21 07:14:25,@UberINSupport @Uber_India  Why you are charging 105 rupees for Toll prices and Fees despite there is no Toll was paid during my whole trip.   I need it refund Immediately ....  https://t.co/kXSOqVku0L,DeveshS07579318,uber,driver\\n2020-12-21 07:14:05,@_prateekbh @Uber congrats Prateek!!,kuldeepkeshwar,uber,driver\\n2020-12-21 07:14:03,\"@AnnCoulter The privileged people who run this uber-posh school feel guilty b/c they\\'re rich and they serve people who are perhaps even wealthier. Rather than face the possible social injustice of \\'classism,\\' which would be disadvantageous to themselves, they redirect attention to \\'racism.\\'\",PoliticsScale,uber,driver\\n2020-12-21 07:13:56,Uber invests in Poland  https://t.co/Cx6GCu2XbN,RagnarNowak,uber,driver\\n2020-12-21 07:13:55,@AnonPig That makes a lot of sense now knowing that he didn’t actually open up a bunch of shops in a global pandemic. Its a common practice among chain restaurants to sell foods on Uber Eats etc under pseudonyms. Chuck E Cheese sells pizza as Pasqually’s on food delivery apps for example,larsereth,uber,driver\\n2020-12-21 07:13:49,O uber deu 150 mas ela não liga porque é o pai que banca,leocpx1,uber,driver\\n2020-12-21 07:13:48,@LaloElizondoTV @UberEats_mex @Uber @Profeco De que me perdí???,naomi_eunki,uber,driver\\n2020-12-21 07:13:47,\"@Uber_Support Hello Uber, I have a few questions about why my application for driving for uber is taking so long\",AnthonyLeeTitus,uber,driver\\n2020-12-21 07:13:30,\"Se o uber chegar fudeu ainda bem q nn pedi nd, tô podendo perder dinheiro pra ngm 🙅🏽\\u200d♀️😌\",jooiararissimaa,uber,driver\\n2020-12-21 07:13:09,Son un maldito asco @UberEats_mex @Uber ojalá @Profeco multará o suspendiera esta aplicación por incumplimientos y mal trato al cliente,LaloElizondoTV,uber,driver\\n2020-12-21 07:13:07,@mgmgmpmgd お店側は30%以上支払うからUberで配達すればするほど赤字になるw（自分ん所で配達員雇うとそれぐらいかかるらしいから決して高い方ではないらしい）  雇用を作ってくれたのは良いこと！（Uberは成果報酬、ライバルの出前館は時給制）,ko_crin_sub,uber,driver\\n2020-12-21 07:12:56,@CambiBrown @mattty_k @JuanWeek I was telling the guys how on Murillo occasions I’ve ordered Uber eats and fell asleep....one time even ordering twice and falling asleep on both orders separately,Donny_Deal,uber,driver\\n2020-12-21 07:12:48,\"Бирюльки №589. Про измерение кислорода в крови и другие параметры    Почему измерение кислорода в крови неточно, как мерить давление часами; Uber в убытках; Google и Apple против COVID; AirPods Pro и тре...  https://t.co/LElScGG3t5, Смартфоны  https://t.co/mvguREKx6O  https://t.co/YQbPDVVeY8\",4TeXno,uber,driver\\n2020-12-21 07:12:45,Oh my god... forgot my brand new phone inside the Uber MINUTES before boarding my train for vacation,DrunkyGa,uber,driver\\n2020-12-21 07:12:43,uberのこのスピード感まじ有難いw  https://t.co/Hyruk42a6f,PUBG_Ryuki,uber,driver\\n2020-12-21 07:12:29,@Uber_Support You guys charged me Rs20 for this both times😡😡i wish consumer court in india is strong enough to file a case,ashwinchill,uber,driver\\n2020-12-21 07:12:24,I know it\\'s possible to delegate all that. Might just fire up a game as an uber rich club in a non-competitive league and focus on the games alone and delegate every single other thing.,NotMetaphoric,uber,driver\\n2020-12-21 07:12:23,I’m cracking up in the back of this uber and I know this driver is scared 💀,Who_ThatPokemon,uber,driver\\n2020-12-21 07:12:19,@Bunny45564943 @macsween_prue @DanielAndrewsMP Did they also have the p/t Uber drivers fu**ing the patrons? Have glucose test strips used on multiple individuals? Guards laying on the floors in gloves sleeping? No training? Outdoor adventures to shopping malls???,katy3710,uber,driver\\n2020-12-21 07:12:14,@Uber_minmin お疲れ様です！ 今日はみたくんと久しぶりに会ったので遊んでましたw そこへみんみんさんが通り過ぎて行ったっすw,machidauver,uber,driver\\n2020-12-21 07:11:54,Uber配達員でヘルメット被ってる人はほんと安心する,wa_riko,uber,driver\\n2020-12-21 07:11:44,برطانیہ کی معروف آن لائن ٹیکسی سروس اب بہت جلد آپ کے شہر لاہور میں #texi #cab #lahore #service #Captain #Uber #Careem #rider #discount #pleasure #school #fun #office #parks #stand #driver #jobsinlahore #LHR #WhatsApp #سستی_پیرنی #یہ_منہ_اور_مسعود_کی_دال #لفافہ_صحافت_بغض_ریاست  https://t.co/CCRkh58W78,MyLiftLhr,uber,driver\\n2020-12-21 07:11:44,@Uber_Support day by day your customer support and experience is getting worst and worst. You have hired unprofessional and illiterate drivers in your company who don’t know how to deal with customers. I think your company is busy making instead of customer support.,RadheSingu,uber,driver\\n2020-12-21 07:11:39,@kimiring621 作ってあげるょ|ू•ω•)ﾁﾗｯ💕 Uber Eatsで運んでくれるかな？,Aki_nenko,uber,driver\\n2020-12-21 07:11:20,@_prateekbh @Uber Congrats bro,lucifierpraveen,uber,driver\\n2020-12-21 07:11:20,結局末端の人に利益がある仕組みなら、中間や上位のステークホルダーにも然るべき利益が分配されるわけよな。業界構造自体が異質（個人的な感想ではUber Eatsとか）だったり変な利権で歪になってたりしたら別だけど。,icedoll17,uber,driver\\n2020-12-21 07:11:14,\"Opinion | Uber, Instacart and others’ vaccine hypocrisy - The Washington Post  https://t.co/d3UIvAj8qP\",JamesTobyne,uber,driver\\n2020-12-21 07:11:00,@kogitaro_uber @ubaosandesu パンツがカレーパンって香辛料でしみないか？ヒリヒリしないか？(´；ω；｀),kikokicoki_ko,uber,driver\\n2020-12-21 07:10:55,@Uber_KSA لو سمحتو سيارتي ٢٠١٣؟(اقدر اشتغل عليها,A1657825024,uber,driver\\n2020-12-21 07:10:31,寒〜い冬は、ウーバーイーツでぬくぬくおうちご飯⛄️❄️ Uber Eats初回限定クーポン✨🉐 注文画面でこちらのコード 【eats-trtrl8】を入力することで、初回のご注文が800円割引となり大変お得です✨🙆🏻\\u200d♀️ #UberEats #ウーバーイーツ,uberwaribiki,uber,driver\\n2020-12-21 07:10:28,lmao not the Uber driver watching me shift my wig around,naomiamorr,uber,driver\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    }
  ]
}